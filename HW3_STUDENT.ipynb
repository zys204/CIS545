{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3_STUDENT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zys204/CIS545/blob/main/HW3_STUDENT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XEqGpEGBWs5"
      },
      "source": [
        "## Step 0: Set up EMR\n",
        "\n",
        "Follow the [AWS Academy Getting Started](https://docs.google.com/document/d/e/2PACX-1vSSv2wg7DVLxqs-8N2zAPsqAvHEhXri4GCo7S8aVFjRtBNIxbpBLFAfZ7KX3hbyT9jaruoE2jOh8EtU/pub) instructions.  You'll need the ones for EMR and connecting to EMR on Colab. You can ignore the section about SageMaker (we'll use that in Homework 5).\n",
        "\n",
        "Move on to Step 0.1 after you have completed all the steps in the document.\n",
        "\n",
        "![ACME GIANT RUBBER BAND](https://pbs.twimg.com/media/DRqbJh7UMAE2z4o?format=jpg&name=4096x4096)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVMCjEV1IEC7"
      },
      "source": [
        "# CIS 545 Homework 3: Spark SQL\n",
        "\n",
        "#### **Worth 100 points (plus 5 points extra credit)**\n",
        "\n",
        "Welcome to CIS 545 Homework 3! In this homework you will gain a mastery of using Spark SQL. By the end, you'll be a star (not that you aren't already one). Over the next few days you will be using an EMR cluster to use Spark to manipulate the  `linkedin_small_real.json` dataset  as well as the `stocks.csv`.\n",
        "\n",
        "The goal of the homework will be to create a training dataset for a Random Forest Machine learning model.  Yes, you'll be playing with machine learning shortly!\n",
        "\n",
        "The training data set will contain the monthly number of employees hired by companies in `linkedin_small_real.json` and their corresponding closing stock prices over a 10 year period (2000-2011). We will try and predict, based on this data, if the company will have a positive or negative growth in stock in the first quarter of the next year. Who's ready to make some money?\n",
        "\n",
        "## The Necessary Notes and Nags\n",
        "Before we begin here are some important notes to keep in mind,\n",
        "\n",
        "\n",
        "1.   **IMPORTANT!** I said it twice, it's really important. In this homework, we will be using AWS resources. You are given a quota ($100) to use for the entirety of the homework. There is a small chance you will use all this money, however it is important that at the end of every session, you **shut down your EMR cluster**.\n",
        "2.   **Be sure you use Google Colab for this Homework** since we must connect to the EMR cluster and local Jupyter will have issues doing that. Using a Google Colab Notebook with an EMR cluster has two important abnormalities:\n",
        "    * The first line of any cell in which you will use the spark session must be `%%spark`. Notice that all cells below have this.\n",
        "    * You will, unfortunately, not be able to stop a cell while it is running. If you wish to do so, you will need to restart your cluster. See the Setup EMR Document for reference.\n",
        "3.   You are **required** to use Spark SQL queries to handle the data in the assignment. Mastering SQL is more beneficial than being able to use Spark commands (functions) as it will show up in more areas of programming and data science/analytics than just Spark. Use the following [function list](https://spark.apache.org/docs/latest/api/sql/index.html#) to see all the SQL functions avaliable in Spark.\n",
        "4.   Throughout the homework you will be manipulating Spark dataframes (sdfs). We do not specify any ordering on the final output. You are welcome to order your final tables in whatever way you deem fit. We will conduct our own ordering when we grade.\n",
        "5. Based on the challenges you've faced in the previous homework, we are including information on the expected schema of your results.  Apache Spark is very fiddly but we hope this will help.\n",
        "6. There are portions of this homework that are _very_ hard. We urge you start early to come to office hours and get help if you get stuck. But don't worry, I can see the future, and you all got this.\n",
        "\n",
        "With that said, let's dive in.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YOtx5e8RvDu"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN0MJZvURvae"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iBPXxgAdXkv"
      },
      "source": [
        "### Step 0.1: The Superfluous Setup\n",
        "\n",
        "Run the following two cells. These will allow your colab notebook to connect to an use your EMR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvkEbVaaAQ1e"
      },
      "source": [
        "%%capture\n",
        "!apt update\n",
        "!apt install gcc python-dev libkrb5-dev\n",
        "!pip install sparkmagic\n",
        "!pip install penngrader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WAJmQ8IAbRs"
      },
      "source": [
        "%%capture\n",
        "%load_ext sparkmagic.magics "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL6n768EPt9E"
      },
      "source": [
        "### Step 0.2: The Sharp Spark\n",
        "\n",
        "Now, connect your notebook to the EMR cluster you created. In the first cell, copy the link to the Master Public DNS specified in the setup document. You will need to add `http://` to the beginning of the address and the port to the end. The final format should be,\n",
        "\n",
        "`http://<your-DNS-link>:8998`\n",
        "\n",
        "For example, if my DNS (directly from the AWS EMR console) is `ec2-3-15-237-211.us-east-2.compute.amazonaws.com` my address would be,\n",
        "\n",
        "`http://ec2-3-15-237-211.us-east-2.compute.amazonaws.com -a cis545-livy -p password1 -t Basic_Access`\n",
        "\n",
        "Insert this in the `# TODO # below`. For our example, the cell would read,\n",
        "\n",
        "```\n",
        "%spark add -s spark_session -l python -u http://ec2-3-15-237-211.us-east-2.compute.amazonaws.com -a cis545-livy -p password1 -t Basic_Access\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9QbylT-jqX9"
      },
      "source": [
        "# TODO: Copy the line above, enter your Master Public DNS with the proper formatting and host, and update the password\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwKAHhQL0lf7"
      },
      "source": [
        "# If you ever need to restart, you may need to...\n",
        "# %spark delete -s my_session\n",
        "#OR just factory reset runtime under the runtime tab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yzJetbCfgh3"
      },
      "source": [
        "### Step 0.3: Cluster Log\n",
        "\n",
        "In order to keep track of clusters you have created and terminated as well as give us information about time spent on this assignment, please enter each date and time you created a cluster and the date and time you terminated the cluster. This will not impact your score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "powXPhtphhyy"
      },
      "source": [
        "EX: \n",
        "\n",
        "10/12 9:00am - 10/12 12:00pm\n",
        "\n",
        "10/13 7:00pm - 10/13 9:00pm\n",
        "\n",
        "...\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SokSZv4vih0c"
      },
      "source": [
        "TODO: Create cluster log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1IQjUwNObb8"
      },
      "source": [
        "Enter your 8-digit Penn Key as an integer in the cell \n",
        "below. This will be used in the autograder.  **Please also update the cell below, with the same ID!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zM20juwqqQF"
      },
      "source": [
        "from penngrader.grader import *\n",
        "STUDENT_ID = #TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8Oo_5D7qoWp"
      },
      "source": [
        "grader = PennGrader(homework_id = 'CIS545_MCITO_Fall_2021_HW3', student_id = STUDENT_ID)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvl5MJg6cLHr"
      },
      "source": [
        "**Please make sure you also update this one, so the grader can similarly be updated on Spark/EMR!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NQjHLyKcL5H"
      },
      "source": [
        "%%spark\n",
        "from penngrader.grader import *\n",
        "STUDENT_ID = 00000000\n",
        "grader = PennGrader(homework_id = 'CIS545_MCITO_Fall_2021_HW3', student_id = STUDENT_ID)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bfs-EQZUzF4j"
      },
      "source": [
        "Run the following cell to setup the autograder, make sure to have set your 8 digit Penn ID in the cell above. It will also import all the modules you need for the homework.\n",
        "\n",
        "_Note_: Since we are using an EMR cluster we will only have access to some of modules that exist for Python, meaning things like `pandas`, `numpy`, etc. may not all be available. We have written the entire homework such that the solution does not require any of these."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf_ADEXnIK0b"
      },
      "source": [
        "## Step 1: Data Wrangling, Cleaning, and Shaping\n",
        "\n",
        "![Data Wrangler](http://jacobjwalker.effectiveeducation.org/blog/wp-content/uploads/2017/05/Data-Wrangler-Gaucho.png)\n",
        "\n",
        "The data you will use is stored in an S3 bucket, a cloud storage service. You now need to download it onto the nodes of your [EMR cluster](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html). \n",
        "\n",
        "### Step 1.1: The Stupendous Schema\n",
        "\n",
        "When loading data, Spark will try to infer it's structure on it's own. This process is faulty because it will sometimes infer the type incorrectly. JSON documents, like the one we will use, can have nested types, such as: arrays, arrays of dictionaries, dictionaries of dictionaries, etc. Spark's ability to determine these nested types is not reliable, thus you will define a schema for `linkedin_small_real.json`.\n",
        "\n",
        "A schema is a description of the structure of data. You will be defining an explicit schema for `linkedin_small_real.json`. In Spark, schema's are defined using a `StructType` object. This is a collection of data types, termed `StructField`'s, that specify the structure and variable type of each component of the dataset. For example, suppose we have the following simple JSON object,\n",
        "\n",
        "\n",
        "```\n",
        "{\n",
        " \"student_name\": \"Data Wrangler\",\n",
        " \"GPA\": 1.4,\n",
        " \"courses\": [\n",
        "    {\"department\": \"Computer and Information Science\",\n",
        "     \"course_id\": \"CIS 545\",\n",
        "     \"semester\": \"Fall 2019\"},\n",
        "    {\"department\": \"Computer and Information Science\",\n",
        "     \"course_id\": \"CIS 555\",\n",
        "     \"semester\": \"Fall 2019\"}\n",
        " ],\n",
        " \"grad_year\": 2021\n",
        " }\n",
        "```\n",
        "\n",
        "We would define its schema as follows,\n",
        "\n",
        "```       \n",
        "schema = StructType([\n",
        "           StructField(\"student_name\", StringType(), nullable=True),\n",
        "           StructField(\"GPA\", FloatType(), nullable=True),\n",
        "           StructField(\"courses\", ArrayType(\n",
        "                StructType([\n",
        "                  StructField(\"department\", StringType(), nullable=True),\n",
        "                  StructField(\"course_id\", StringType(), nullable=True),\n",
        "                  StructField(\"semester\", StringType(), nullable=True)\n",
        "                ])\n",
        "           ), nullable=True),\n",
        "           StructField(\"grad_year\", IntegerType(), nullable=True)\n",
        "         ])\n",
        "```\n",
        "\n",
        "\n",
        "Each `StructField` has the following structure: `(name, type, nullable)`. The `nullable` flag defines that the specified field may be empty. Your first task is to define the `schema` of `linkedin_small_real.json`. A smaller version of the JSON dataset can be found [here](https://drive.google.com/a/seas.upenn.edu/file/d/1yZ_0xz6uSJ8lAxhGzn2BVjCpDOjagcqb/view?usp=sharing).\n",
        "\n",
        "_Note_: In `linkedin_small_real.json` the field `specilities` is spelled incorrectly. This is **not** a typo. (Well, it is, but it's a typo in the raw data, and we have to live with what we're given.)\n",
        "\n",
        "There is also no grading cell for this step.  But your JSON file won't load if it's wrong, so you have a way of testing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL-Ps4KWIJ9e"
      },
      "source": [
        "%%spark\n",
        "\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# TODO: Finish defining the linkedin_small_real.json schema\n",
        "# We've provided most of the fiddly details, but you'll\n",
        "# need to fill in the **name** and the **experience**!\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"_id\", StringType(), nullable=True),\n",
        "    StructField(\"education\", ArrayType(\n",
        "      StructType([\n",
        "          StructField(\"start\", StringType(), nullable=True),\n",
        "          StructField(\"major\", StringType(), nullable=True),\n",
        "          StructField(\"end\", StringType(), nullable=True),\n",
        "          StructField(\"name\", StringType(), nullable=True),\n",
        "          StructField(\"desc\", StringType(), nullable=True),\n",
        "          StructField(\"degree\", StringType(), nullable=True)\n",
        "      ])\n",
        "    ), nullable=True),\n",
        "    StructField(\"group\", StructType([\n",
        "          StructField(\"affilition\", ArrayType(StringType()), nullable=True),\n",
        "          StructField(\"member\", StringType(), nullable=True)\n",
        "    ]), nullable=True), \n",
        "\n",
        "    # TODO: fill in the necessary structure for the name (Don't forget the comma at the end)!\n",
        "\n",
        "\n",
        "    StructField(\"locality\", StringType(), nullable=True),\n",
        "    StructField(\"skills\", ArrayType(StringType()), nullable=True),\n",
        "    StructField(\"industry\", StringType(), nullable=True),\n",
        "    StructField(\"interval\", IntegerType(), nullable=True),\n",
        "\n",
        "    # TODO: fill in structure for experience (Don't forget the comma at the end)!\n",
        "\n",
        "\n",
        "    StructField(\"summary\", StringType(), nullable=True),\n",
        "    StructField(\"interests\", StringType(), nullable=True),\n",
        "    StructField(\"overview_html\", StringType(), nullable=True),\n",
        "    StructField(\"specilities\", StringType(), nullable=True),\n",
        "    StructField(\"homepage\", ArrayType(StringType()), nullable=True),\n",
        "    StructField(\"honors\", ArrayType(StringType()), nullable=True),\n",
        "    StructField(\"url\", StringType(), nullable=True),\n",
        "    StructField(\"also_view\", ArrayType(\n",
        "      StructType([\n",
        "          StructField(\"id\", StringType(), nullable=True),\n",
        "          StructField(\"url\", StringType(), nullable=True)\n",
        "      ])\n",
        "    ), nullable=True),\n",
        "    StructField(\"events\", ArrayType(\n",
        "      StructType([\n",
        "          StructField(\"from\", StringType(), nullable=True),\n",
        "          StructField(\"to\", StringType(), nullable=True),\n",
        "          StructField(\"title1\", StringType(), nullable=True),\n",
        "          StructField(\"start\", IntegerType(), nullable=True),\n",
        "          StructField(\"title2\", StringType(), nullable=True),\n",
        "          StructField(\"end\", IntegerType(), nullable=True)\n",
        "      ])), nullable=True)\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Su604X9ggc2"
      },
      "source": [
        "### Step 1.2: The Langorous Load\n",
        "\n",
        "Load the `linkedin_small_real.json` dataset from your S3 bucket into a Spark dataframe (sdf) called `raw_data_sdf`. If you have constructed `schema` correctly `spark.read.json()` will read in the dataset. ***You do not need to edit this cell***.\n",
        "\n",
        "If this doesn't work, go back to the prior cell and update your `schema`!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji-KW2sAiB6r"
      },
      "source": [
        "%%spark\n",
        "\n",
        "raw_data_sdf = spark.read.json(\"s3://penn-cis545-files/linkedin_small_real.json\", schema=schema)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvurIgjLkdZL"
      },
      "source": [
        "%%spark\n",
        "raw_data_sdf.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMVCVotcE1wv"
      },
      "source": [
        "The cell below shows how to run SQL commands on Spark tables. Use this as a template for all your SQL queries in this notebook. ***You do not need to edit this cell***."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJSVWeGiEO5c"
      },
      "source": [
        "%%spark\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Create SQL-accesible table\n",
        "raw_data_sdf.createOrReplaceTempView(\"raw_data\")\n",
        "\n",
        "# Declare SQL query to be excecuted\n",
        "query = '''SELECT * \n",
        "           FROM raw_data ORDER BY _id LIMIT 10'''\n",
        "\n",
        "# Save the output sdf of spark.sql() as answer_sdf and convert to Pandas\n",
        "answer_sdf = spark.sql(query)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLF-swRBTuZG"
      },
      "source": [
        "We will copy the `answer_sdf` to Colab to submit to PennGrader..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JL2FZp8TsH_"
      },
      "source": [
        "%spark -o answer_sdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tanLAYVq2Hn"
      },
      "source": [
        "## AUTOGRADER Step 1.2: ##\n",
        "\n",
        "grader.grade(test_case_id = 'first', answer = answer_sdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "705XndyQYW6f"
      },
      "source": [
        "In the next cell, create `step_1_2_sdf` to fetch the data from the above table, returning rows with schema `(_id, given_name)`, in **lexicographical order** of `_id`.  (Note `_id` is a string as opposed to an int.) Limit your sdf to 10 rows. Save your final answer to a variable to_submit as shown above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOpHsWR2qQAY"
      },
      "source": [
        "%%spark \n",
        "\n",
        "# TODO\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcw5V53sT4oN"
      },
      "source": [
        "%spark -o to_submit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4hU1ihWYjL6"
      },
      "source": [
        "grader.grade(test_case_id = 'lex_10_ids_last_names', answer = to_submit)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svOO4iLPist4"
      },
      "source": [
        "### Step 1.3: The Extravagant Extraction\n",
        "\n",
        "In our training model, we are interested in when individuals began working at a company.  From creating the schema, you should notice that the collection of companies inviduals worked at are contained in the `experience` field as an array of dictionaries. You should use the `org` for the company name and `start` for the start date. Here is an example of an `experience` field,\n",
        "\n",
        "```\n",
        "{\n",
        "   \"experience\": [\n",
        "     {\n",
        "        \"org\": \"The Walt Disney Company\", \n",
        "        \"title\" : \"Mickey Mouse\",\n",
        "        \"end\" : \"Present\",\n",
        "        \"start\": \"November 1928\",\n",
        "        \"desc\": \"Sailed a boat.\"\n",
        "     },\n",
        "     {\n",
        "        \"org\": \"Walt Disney World Resort\",\n",
        "        \"title\": \"Mickey Mouse Mascot\",\n",
        "        \"start\": \"January 2005\",\n",
        "        \"desc\": \"Took pictures with kids.\"\n",
        "     }\n",
        "   ]\n",
        "}\n",
        "```\n",
        "\n",
        "Your task is to extract each pair of company and start date from these arrays. In Spark, this is known as \"exploding\" a row. If you think about how we used relational data to model a nested list in a separate table -- that's basically what `explode` does to the nested data within `linkedin`.\n",
        "\n",
        "Create an sdf called `raw_start_dates_sdf` that contains the company and start date for every experience of every individual in `raw_data_sdf`. Drop any row that contains a `null` in either column with `dropna()`. Remember we will sort the dataframe when grading so you can sort the elements however you wish (you don't need to if you don't want to). The sdf should look similar to:\n",
        "\n",
        "```\n",
        "+--------------------------+---------------+\n",
        "|org                       |start_date     |\n",
        "+--------------------------+---------------+\n",
        "|Walt Disney World Resort  |January 2005   | \n",
        "|The Walt Disney Company   |November 1928  |\n",
        "|...                       |...            |\n",
        "+--------------------------+---------------+\n",
        "```\n",
        "\n",
        "_Hint_: You may want to do two separate `explode`s for `org` and `start`. In an explode, the position of the element in the array can be extracted as well, and used to merge two seperate explodes. Reference the [function list](https://spark.apache.org/docs/2.3.0/api/sql/index.html).\n",
        "\n",
        "_Note_: Some of the entires in `org` are \"weird\", i.e. made up of non-english letters and characters. Keep them. **DO NOT** edit any name in the original dataframe unless we specify. **DO NOT** drop any row unless there is a `null` value as stated before. This goes for the rest of the homework as well, unless otherwise specified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt16tyP0klQX"
      },
      "source": [
        "%%spark\n",
        "\n",
        "# TODO: Create [raw_start_dates_sdf]\n",
        "\n",
        "# POSEXPLODE() will explode a specified column of an sdf and return two rows\n",
        "# corresponding to the index of an element in an array \"pos\" and the element\n",
        "# itself.\n",
        "\n",
        "############################################\n",
        "\n",
        "# Explode the org array in experience\n",
        "\n",
        "\n",
        "# Save as a SQL-accesible table called \"orgs\"\n",
        "\n",
        "\n",
        "# Explode the start_date array in experience\n",
        "\n",
        "\n",
        "# Join each explode based on a person's id and the index of each element in the\n",
        "# original array\n",
        "\n",
        "\n",
        "# Define and save raw_start_dates_sdf\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-a3xLDEYCcwl"
      },
      "source": [
        "%%spark\n",
        "\n",
        "# For your info, see if it looks reasonable\n",
        "raw_start_dates_sdf.show(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKV_hk7gDX9l"
      },
      "source": [
        "%%spark \n",
        "\n",
        "## AUTOGRADER Step 1.3: ##\n",
        "\n",
        "raw_start_dates_sdf.createOrReplaceTempView(\"test_1_3\")\n",
        "test_1_3_sdf = spark.sql(\"SELECT * FROM test_1_3 ORDER BY org ASC, start_date DESC LIMIT 10\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjJebz8IUAoL"
      },
      "source": [
        "%spark -o test_1_3_sdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTukZvW9Pj04"
      },
      "source": [
        "grader.grade(test_case_id = 'explosion', answer = test_1_3_sdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSbb0t-d-VFt"
      },
      "source": [
        "### Step 1.4: The Fortuitous Formatting\n",
        "\n",
        "There are two issues with the values in our `date` column. First, the values are saved as strings, not datetime types. This halts us from running functions such as `ORDER BY` or `GROUP BY` on common months or years. Second, some values do not have both month and year information or are in other languages. Your task is to filter out and clean the `date` column. We are interested in only those rows that have date in the following format \"(month_name) (year)\", e.g. \"October 2010\".\n",
        "\n",
        "Create an sdf called `filtered_start_dates_sdf` from `raw_start_dates_sdf` with the `date` column filtered in the manner above. Keep only those rows with a start date between January 2000 to December 2011, inclusive. Ensure that any dates that are not in our desired format are ommitted. Drop any row that contains a `null` in either column. The format of the sdf is shown below:\n",
        "```\n",
        "+--------------------------+---------------+\n",
        "|org                       |start_date     |\n",
        "+--------------------------+---------------+\n",
        "|Walt Disney World Resort  |2005-01-01     | \n",
        "|...                       |...            |\n",
        "+--------------------------+---------------+\n",
        "```\n",
        "_Hint_: Refer to the [function list](https://spark.apache.org/docs/2.3.0/api/sql/index.html) to format the `date` column. In Spark SQL the date format we are interested in is `\"MMMM y\"`.\n",
        "\n",
        "_Note_: Spark will return the date in the format above, with the day as `01`. This is ok, since we are interested in the month and year each individual began working and all dates will have `01` as their day."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eelgTtOc_MBM"
      },
      "source": [
        "%%spark\n",
        "\n",
        "# TO_DATE() will convert a string to a datetime object. The string's format,\n",
        "# i.e. \"MMMM y\" must be provided. Any string that does not have the specified\n",
        "# format will be returned as null.\n",
        "\n",
        "# Use TO_DATE() to convert the start_date column from a string to a datetime\n",
        "# object. Keep only dates that are between January 2000 and December 2011,\n",
        "# inclusive.\n",
        "\n",
        "\n",
        "# TODO: Create [filtered_start_dates_sdf]\n",
        "\n",
        "\n",
        "# Define and save filtered_start_dates_sdf\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx-g8FY0GeLA"
      },
      "source": [
        "%%spark\n",
        "\n",
        "## AUTOGRADER Step 1.4: ##\n",
        "\n",
        "filtered_start_dates_sdf.createOrReplaceTempView(\"test_1_4\")\n",
        "test_1_4_sdf = spark.sql(\"SELECT org, DATE_FORMAT(start_date, 'yyyy-MM-dd') AS start_date FROM test_1_4 ORDER BY org DESC, start_date DESC LIMIT 20\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLCEZNDEUGrk"
      },
      "source": [
        "%spark -o test_1_4_sdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVyAg4wmQszL"
      },
      "source": [
        "grader.grade(test_case_id = 'fortuitous', answer = test_1_4_sdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXYYQn2_GYwZ"
      },
      "source": [
        "### Step 1.5 The Gregarious Grouping\n",
        "\n",
        "We now want to collect the number of individuals that started in the same month and year for each company. Create an sdf called `start_dates_sdf` that has the total number of employees who began working at the same company on the same start date. The format of the sdf is shown below:\n",
        "\n",
        "```\n",
        "+--------------------------+---------------+---------------+\n",
        "|org                       |start_date     |num_employees  |\n",
        "+--------------------------+---------------+---------------+\n",
        "|Walt Disney World Resort  |2005-01-01     |1              |\n",
        "|...                       |...            |...            |\n",
        "+--------------------------+---------------+---------------+\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxVIyc1CHooV"
      },
      "source": [
        "%%spark\n",
        "\n",
        "# TODO: Create [start_dates_sdf]\n",
        "\n",
        "# GROUP BY on org and start_date, in that order.\n",
        "\n",
        "\n",
        "# Define and save start_dates_sdf\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aI7N3JgIMFeV"
      },
      "source": [
        "%%spark\n",
        "\n",
        "## AUTOGRADER Step 1.5: ##\n",
        "\n",
        "start_dates_sdf.createOrReplaceTempView(\"test_1_5\")\n",
        "test_1_5_sdf = spark.sql(\"SELECT org, DATE_FORMAT(start_date, 'yyyy-MM-dd') as start_date, num_employees FROM test_1_5 ORDER BY num_employees DESC, org DESC, start_date ASC LIMIT 10\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaw1fSoFUKUS"
      },
      "source": [
        "%spark -o test_1_5_sdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bWO9dx1UYux"
      },
      "source": [
        "grader.grade(test_case_id = 'gregarious', answer = test_1_5_sdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYScM3FwJnUz"
      },
      "source": [
        "## Step 2: Hiring Trends Analysis\n",
        "\n",
        "Now we will analyze `start_dates_sdf` to find monthly and annual hiring trends.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCcU96OwwqT1"
      },
      "source": [
        "\n",
        "### Step 2.1: The Marvelous Months\n",
        "\n",
        "Your task is to answer the question: \"What is the **most popular month** for employees to start working?\" Create an sdf called `monthly_hires_sdf` which contains the total number of employees that started working on a specific month, at any company and on any year. The `month` column should be of type `int`, i.e. 1-12. The format of the sdf is shown below:\n",
        "\n",
        "```\n",
        "+---------------+---------------+\n",
        "|month          |num_employees  |\n",
        "+---------------+---------------+\n",
        "|1              |...            |\n",
        "|2              |...            |\n",
        "|3              |...            |\n",
        "|...            |...            |\n",
        "+---------------+---------------+\n",
        "```\n",
        "\n",
        "Find the month in which the *most* employees start working and save its number as an **integer** to the variable `most_common_month`. This can be submitted by hardcoding the month to the variable, but we encourage you to try it without hardcoding the integer of the month. \n",
        "\n",
        "_Hint_: Be careful. The start dates we have right now have both month and year. We only want the common months. See if you can find something in the [function list](https://spark.apache.org/docs/2.3.0/api/sql/index.html) that will help you do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLTmvD9WNQH3"
      },
      "source": [
        "%%spark\n",
        "\n",
        "# TODO: Create [monthly_hire_sdf] and find the most common month people were\n",
        "# hired. Save its number as an integer to [most_common_month]\n",
        "\n",
        "# HINT: You can aggregate by month, and use ordering to figure out the\n",
        "# most or least common.\n",
        "\n",
        "# MONTH() will return the month for any datetime object.\n",
        "\n",
        "# GROUP BY on the month of start_date using MONTH(). Sum the num_employees\n",
        "# column to find the total number of employees that started on that month\n",
        "\n",
        "\n",
        "# Define and save monthly_hires_sdf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCrtcH-KieIM"
      },
      "source": [
        "%%spark\n",
        "\n",
        "## AUTOGRADER Step 2.1: ##\n",
        "\n",
        "monthly_hires_sdf.createOrReplaceTempView(\"test_2_1\")\n",
        "test_2_1_sdf = spark.sql(\"SELECT * FROM test_2_1 ORDER BY month ASC\")\n",
        "test_2_1_sdf.show()\n",
        "print(most_common_month)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLZCgBDdUPnp"
      },
      "source": [
        "%spark -o test_2_1_sdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6Ot6rFKUrH3"
      },
      "source": [
        "grader.grade(test_case_id = 'marvelous', answer = test_2_1_sdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUtVHRcMUoWp"
      },
      "source": [
        "### Step 2.2: The Preposterous Perennial Percentages\n",
        "\n",
        "The next question we will answer is \"What is the percentage change in hires between 2009 and 2010 for each company?\" Create an sdf called `percentage_change_sdf` that has the percentage change between 2009 and 2010 for each company. The sdf should look as follows:\n",
        "\n",
        "```\n",
        "+---------------------------+--------------------+\n",
        "|org                        |percentage_change   |\n",
        "+---------------------------+--------------------+\n",
        "|Walt Disney World Resort   |12.3                |\n",
        "|...                        |...                 |\n",
        "+---------------------------+--------------------+\n",
        "```\n",
        "\n",
        "_Note_: A percentage change can be positive or negative depending \n",
        "on the difference between the two years. The formula for percent change is given below,\n",
        "\n",
        "$$\\text{% change} = \\frac{P_f-P_i}{P_i} \\times 100$$\n",
        "\n",
        "Here, $P_f$ is the final element (in this case the number of hires in 2010) and $P_i$ is initial element (the number of hires in 2009).\n",
        "\n",
        "_Hint_: This is a **nontrivial** question and involves you putting the 2009 and 2010 data on the same row. I'm really sorry it isn't easier, but that's why you are in the class (I hope!). We recommend using a combination of `GROUP BY` and `JOIN`. Keep in mind that operations between columns in SQL dataframes are often easier than those between rows. Come to office hours if you need help."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AhhfLXpWq7y"
      },
      "source": [
        "%%spark\n",
        "\n",
        "# TODO: Create [percentage_change_sdf]\n",
        "\n",
        "# YEAR() will return the year of a datetime object.\n",
        "\n",
        "# The column percentage_change is calculated by doing a JOIN. We join an sdf\n",
        "# \"y1\" created by doing a GROUP BY on start_dates by org and the year of\n",
        "# start_date where the year is 2009. We sum the number of employees. That sdf is\n",
        "# joined with a similar sdf, \"y2\", but for the case when year is 2010. Then,\n",
        "# using the formula in the description above we find the percentage_change of\n",
        "# the two sum columns.\n",
        "\n",
        "\n",
        "\n",
        "# Define and save percentage_change_sdf\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzCb93YH7BcB"
      },
      "source": [
        "%%spark\n",
        "percentage_change_sdf.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3MZT21Qp5Fk"
      },
      "source": [
        "%%spark\n",
        "\n",
        "## AUTOGRADER Step 2.2: ##\n",
        "\n",
        "percentage_change_sdf.createOrReplaceTempView(\"test_2_2\")\n",
        "test_2_2_sdf = spark.sql(\"SELECT * FROM test_2_2 ORDER BY percentage_change DESC, org DESC LIMIT 10\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO7S4QrCUYJ4"
      },
      "source": [
        "%spark -o test_2_2_sdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8i_bc6bU93w"
      },
      "source": [
        "grader.grade(test_case_id = 'preposterous', answer = test_2_2_sdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZIfJGqDqKzX"
      },
      "source": [
        "## The Blessed Break\n",
        "\n",
        "That last question was hard. And it's gonna get harder. Take a break. Sit back and relax for a minute. Listen to some music. Here's a [suggestion](https://www.youtube.com/watch?v=A3yCcXgbKrE).\n",
        "\n",
        "In the cell below fill out the boolean variable `whatd_you_think` with `True` if you liked it or `False` if you didn't. You will be graded on your response."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkA0_E2485jy"
      },
      "source": [
        "whatd_you_think = # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lS-XkWl2deZ"
      },
      "source": [
        "grader.grade(test_case_id = 'tunes', answer = whatd_you_think)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkF2RfLSXO0u"
      },
      "source": [
        "## Step 3: Formatting the Training Data\n",
        "\n",
        "\n",
        "Our overarching goal is to train a machine learning (ML) model that will use the monthly hiring trends of a company to predict a positive or negative gain in the company's stock in the first quarter of the following year. A ML model is trained on a set of observations. Each observation contains a set of features, `X`, and a label, `y`. The goal of the ML model is to create a function that takes any `X` as an input and outputs a predicted `y`. \n",
        "\n",
        "The machine learning model we will use is a [Random Forest Classifier](https://builtin.com/data-science/random-forest-algorithm). Each observation we will pass in will have 24 features (columns). These are the number of people hired from Jan to Dec and the company stock price on the last day of each month. The label will be the direction of the company's stock percentage change (positive, `1`, or negative, `-1`) in the first quarter of the following year. Each observation will correspond to a specified company's trends on a specified year. The format of our final training sdf is shown below. The first 26 columns define our observations, `X`, and the last column the label, `y`.\n",
        "```\n",
        "+----+-----+----------+---------+----------+----------+---------+----------+-------------+\n",
        "|org |year |jan_hired |   ...   |dec_hired |jan_stock |   ...   |dec_stock |stock_result |\n",
        "+----+-----+----------+---------+----------+----------+---------+----------+-------------+\n",
        "|IBM |2009 |...       |   ...   |...       |...       |   ...   |...       |1            |\n",
        "|IBM |2010 |...       |   ...   |...       |...       |   ...   |...       |-1           |\n",
        "|... |...  |...       |   ...   |...       |...       |   ...   |...       |...          |\n",
        "+----+-----+----------+---------+----------+----------+---------+----------+-------------+\n",
        "```\n",
        "\n",
        "_Note_: We will use the first three letters of each month in naming, i.e. `jan, feb, mar, apr, may, jun, jul, aug, sep, oct, nov, dec`.\n",
        "\n",
        "\n",
        "\n",
        "### Step 3.1: The Harmonious Hires\n",
        "\n",
        "Your first task is to create the first half of the training table, i.e. the `jan_hired` through `dec_hired` columns. This will involve reshaping `start_dates_sdf`. Currently, `start_dates_sdf` has columns `org`, `start_date`, and `num_employees`. We want to group the rows together based on common `org` and years and create new columns for the number of employees that started working in each month of that year.\n",
        "\n",
        "Create an sdf called `raw_hirings_for_training_sdf` that has for a single company and a single year, the number of hires in Jan through Dec, and the total number of hires that year. Note that for each company you will have several rows corresponding to years between 2000 and 2011. It is ok if for a given company you don't have a given year. However, ensure that for a given company and given year, each month column has an entry, i.e. if no one was hired the value should be `0`. The format of the sdf is shown below: \n",
        "```\n",
        "+----+-----+----------+---------+----------+----------+\n",
        "|org |year |jan_hired |   ...   |dec_hired |total_num |\n",
        "+----+-----+----------+---------+----------+----------+\n",
        "|IBM |2008 |...       |   ...   |...       |...       |\n",
        "|IBM |2009 |...       |   ...   |...       |...       |\n",
        "|... |...  |...       |   ...   |...       |...       |\n",
        "+----+-----+----------+---------+----------+----------+\n",
        "```\n",
        "_Hint_: This is a **fiddly and somewhat difficult** question. I'm really really sorry. The tricky part is creating the additional columns of monthly hires, specifically when there are missing dates. In our dataset, if a company did not hire anybody in a given date, it will not appear in `start_dates_sdf`. \n",
        "\n",
        "We suggest you look into `CASE` and `WHEN` statements in the [function list](https://spark.apache.org/docs/2.3.0/api/sql/index.html), and use these to **either** fill in a number for column (if appropriate) or put in a 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btp2wboHqg2J"
      },
      "source": [
        "%%spark\n",
        "\n",
        "# TODO: Create [raw_hire_train_sdf]\n",
        "\n",
        "# CASE() statements are SQL's equivalent of if else statements. WHEN a CASE is\n",
        "# true THEN we define a function. ELSE we do another function and then END the\n",
        "# statement.\n",
        "\n",
        "# The query is a GROUP BY. We group data based on the same company and year, as\n",
        "# in the previous step. We then do a CASE statement. This will seperate out the\n",
        "# sets of data corresponding to the same month using MONTH() in the WHEN clause.\n",
        "# If we have a piece of data, it will be the number of employees that started\n",
        "# working at a given company on a given year and a given month and we will save\n",
        "# it with a corresponding column name. If there is no piece of data here, as per\n",
        "# the question, we need to add a 0. This is the ELSE clause. Lastly, we do a\n",
        "# SUM() to find total_num\n",
        "\n",
        "\n",
        "\n",
        "# Define and save raw_hire_train_sdf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRMSunHVq296"
      },
      "source": [
        "%%spark\n",
        "\n",
        "## AUTOGRADER Step 3.1: ##\n",
        "\n",
        "raw_hire_train_sdf.createOrReplaceTempView(\"test_3_1\")\n",
        "test_3_1_sdf = spark.sql(\"SELECT * FROM test_3_1 ORDER BY org DESC, year ASC LIMIT 10\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_g_bcIdUgNy"
      },
      "source": [
        "%spark -o test_3_1_sdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NluLbb4HX5AS"
      },
      "source": [
        "grader.grade(test_case_id = 'harmonious', answer = test_3_1_sdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkXyet6rrczK"
      },
      "source": [
        "### Step 3.2: The Formidable Filters\n",
        "\n",
        "Create an sdf called `hire_train_sdf` that contains all the observations in `raw_hire_train_sdf` with `total_num` greater than or equal to 100. The format of the sdf is shown below:\n",
        "\n",
        "```\n",
        "+----+-----+----------+---------+----------+----------+\n",
        "|org |year |jan_hired |   ...   |dec_hired |total_num |\n",
        "+----+-----+----------+---------+----------+----------+\n",
        "|IBM |2008 |...       |   ...   |...       |...       |\n",
        "|IBM |2009 |...       |   ...   |...       |...       |\n",
        "|... |...  |...       |   ...   |...       |...       |\n",
        "+----+-----+----------+---------+----------+----------+\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCH4mbNcshq9"
      },
      "source": [
        "%%spark\n",
        "\n",
        "# TODO: Create [hire_train_sdf]\n",
        "\n",
        "# Keep all rows where total_num >= 100\n",
        "\n",
        "\n",
        "\n",
        "# Define and save hire_train_sdf\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqUESMO2f3wS"
      },
      "source": [
        "%%spark\n",
        "\n",
        "hire_train_sdf.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wc-4112zsvfF"
      },
      "source": [
        "%%spark\n",
        "\n",
        "## AUTOGRADER Step 3.2: ##\n",
        "\n",
        "hire_train_sdf.createOrReplaceTempView(\"test_3_2\")\n",
        "test_3_2_sdf = spark.sql(\"SELECT * FROM test_3_2 ORDER BY org DESC, year ASC LIMIT 10\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0sOnpYbUlMD"
      },
      "source": [
        "%spark -o test_3_2_sdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCCwP4ldZBPd"
      },
      "source": [
        "grader.grade(test_case_id = 'formidable', answer = test_3_2_sdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MN4ik70Hta01"
      },
      "source": [
        "\n",
        "### Step 3.3: The Stupendous Stocks\n",
        "\n",
        "Now we are ready for the stock data. The stock data we will use is saved in the same S3 bucket as `linkedin.json`. Load the data into the EMR cluster. Run the cell below. ***You do not need to edit this cell***."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APv4BxKw643q"
      },
      "source": [
        "%%spark\n",
        "\n",
        "# Load stock data\n",
        "\n",
        "raw_stocks_sdf = spark.read.format(\"csv\") \\\n",
        "              .option(\"header\", \"true\") \\\n",
        "              .load(\"s3a://penn-cis545-files/stocks.csv\")\n",
        "\n",
        "# Creates SQL-accesible table\n",
        "raw_stocks_sdf.createOrReplaceTempView('raw_stocks')\n",
        "\n",
        "# Display the first 10 rows\n",
        "query = '''SELECT *\n",
        "           FROM raw_stocks'''\n",
        "spark.sql(query).show(10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUCdr3zDUAFH"
      },
      "source": [
        "Run the cell below to see the types of the columns in our data frame. These are not correct. We could have defined a schema when reading in data but we will handle this issue in another manner. You will do this in Step 3.4.2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNTGEfxsisqs"
      },
      "source": [
        "%%spark \n",
        "\n",
        "# Print types of SDF\n",
        "raw_stocks_sdf.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DdnoFkP7mxz"
      },
      "source": [
        "### Step 3.4 The Clairvoyant Cleaning\n",
        "\n",
        "We now want to format the stock data set into the second half of the training table. We will then merge it with `hire_train` based off the common `org` and `year` fields. The formatting will consist of 4 steps. Actually, it is 5.\n",
        "\n",
        "#### Step 3.4.1 The Ubiquitous UDF\n",
        "\n",
        "The companies in our stock dataset are defined by their stock tickers. Thus, we would not be able to merge it with the `org` field in `hire_train_sdf`. We must convert them to that format. Often times when using Spark, there may not be a built-in SQL function that can do the operation we desired. Instead, we can create one on our own with a user-defined function (udf).\n",
        "\n",
        "A udf is defined as a normal Python function and then registered to be used as a Spark SQL function. Your task is to create a udf, `TICKER_TO_NAME()` that will convert the ticker field in `raw_stocks` to the company's name. This will be done using the provided `ticker_to_name_dict` dictionary. We are only interested in the companies in that dictionary.\n",
        "\n",
        "Fill out the function `ticker_to_name()` below. Then use `spark.udf.register()` to register it as a SQL function. The command is provided. ***You do not need to edit it***. Note, we have defined the udf as returning `StringType()`. Ensure that your function returns this. You must also deal with any potential `null` cases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4cJWZsr8iNC"
      },
      "source": [
        "%%spark\n",
        "\n",
        "# Dictionary linking stock ticker symbols to their names\n",
        "ticker_to_name_dict = {'NOK': 'Nokia',\n",
        "                       'UN': 'Unilever',\n",
        "                       'BP': 'BP',\n",
        "                       'JNJ': 'Johnson & Johnson',\n",
        "                       'TCS': 'Tata Consultancy Services',\n",
        "                       'SLB': 'Schlumberger',\n",
        "                       'NVS': 'Novartis',\n",
        "                       'CNY': 'Huawei',\n",
        "                       'PFE': 'Pfizer',\n",
        "                       'ACN': 'Accenture',\n",
        "                       'DELL': 'Dell',\n",
        "                       'MS': 'Morgan Stanley',\n",
        "                       'ORCL': 'Oracle',\n",
        "                       'BAC': 'Bank of America',\n",
        "                       'PG': 'Procter & Gamble',\n",
        "                       'CGEMY': 'Capgemini',\n",
        "                       'GS': 'Goldman Sachs',\n",
        "                       'C': 'Citi',\n",
        "                       'IBM': 'IBM',\n",
        "                       'CS': 'Credit Suisse',\n",
        "                       'MDLZ': 'Kraft Foods',\n",
        "                       'WIT': 'Wipro Technologies',\n",
        "                       'CSCO': 'Cisco Systems',\n",
        "                       'PWC': 'PwC',\n",
        "                       'GOOGL': 'Google',\n",
        "                       'CTSH': 'Cognizant Technology Solutions',\n",
        "                       'HSBC': 'HSBC',\n",
        "                       'DB': 'Deutsche Bank',\n",
        "                       'MSFT': 'Microsoft',\n",
        "                       'HPE': 'Hewlett-Packard',\n",
        "                       'ERIC': 'Ericsson',\n",
        "                       'BCS': 'Barclays Capital',\n",
        "                       'GSK': 'GlaxoSmithKline'}\n",
        "\n",
        "# TODO: Fill out [ticker_to_name()] and register it as a udf.\n",
        "# Fill out ticker_to_name()\n",
        "\n",
        "# In UDFs we have to cover all possible output cases, or else the function will\n",
        "# crash. Specifically, this means we need to handle the case when \"ticker\" is\n",
        "# not in \"ticker_to_name_dict\". We use a try and except statement to return null\n",
        "# for this case.\n",
        "\n",
        "def ticker_to_name(ticker):\n",
        "\n",
        "  \n",
        "\n",
        "# Register udf as a SQL function. DO NOT EDIT\n",
        "spark.udf.register(\"TICKER_TO_NAME\", ticker_to_name, StringType())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSM0mzPRaKKr"
      },
      "source": [
        "Submit a tuple to the autograder for the ticker value of Google and Tesla. If the ticker value isn't in the table, set it to a string equal to \"None\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkdrFilOtKqx"
      },
      "source": [
        "%%spark\n",
        "\n",
        "## AUTOGRADER Step 3.4.1: ##\n",
        "\n",
        "print((str(ticker_to_name(\"GOOGL\")),str(ticker_to_name(\"TSLA\"))))\n",
        "to_submit = ((str(ticker_to_name(\"GOOGL\")),str(ticker_to_name(\"TSLA\"))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BA95n1lpZ4yy"
      },
      "source": [
        "%%spark\n",
        "grader.grade(test_case_id = 'clairvoyant', answer = to_submit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9YOYO9L-_GS"
      },
      "source": [
        "#### Step 3.4.2: The Fastidious Filters\n",
        "\n",
        "With our new `TICKER_TO_NAME()` function we will begin to wrangle `raw_stocks_sdf`.\n",
        "\n",
        "Create an sdf called `filter_1_stocks_sdf` as follows. Convert all the ticker names in `raw_stocks_sdf` to the company names and save it as `org`. Next, convert the `date` field to a datetime type. As explained before this will help order and group the rows in future steps. Then, convert the type of the values in `closing_price` to `float`. This will take care of the `dtypes` issue we saw in Step 3.3.\n",
        "\n",
        "Drop any company names that do not appear in `ticker_to_name_dict`. Using .dropna() is acceptable instead of IS NOT NULL. Keep any date between January 1st 2001 and December 4th 2012 inclusive, in the format shown below (note this is a datetime object not a string):\n",
        "\n",
        "```\n",
        "+----+------------+--------------+\n",
        "|org |date        |Close         |\n",
        "+----+------------+--------------+\n",
        "|IBM |2000-01-03  |...           |\n",
        "|... |...         |...           |\n",
        "+----+------------+--------------+\n",
        "```\n",
        "_Hint_: You will use a similar function to filter the dates as in Step 1.4. In Spark SQL the format for the `date` field in `raw_stocks_sdf` is `\"yyyy-MM-dd\"`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuiitnWlBYJ7"
      },
      "source": [
        "%%spark\n",
        "\n",
        "# Format the \"org\" column using our UDF, TICKER_TO_NAME. Use TO_DATE() to\n",
        "# convert the string date column to datetime object and filter on this in the\n",
        "# same way as Step 1.4\n",
        "\n",
        "# TODO: Create [filter_1_stocks_sdf]\n",
        "\n",
        "\n",
        "\n",
        "# Define and save filter_1_stocks_sdf\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBm-D6FXtdv6"
      },
      "source": [
        "%%spark\n",
        "\n",
        "## AUTOGRADER Step 3.4.2: ##\n",
        "\n",
        "filter_1_stocks_sdf.createOrReplaceTempView(\"test_3_4_2\")\n",
        "test_3_4_2_sdf = spark.sql(\"SELECT org, DATE_FORMAT(date, 'yyyy-MM-dd') as date, Close FROM test_3_4_2 ORDER BY org, date, Close LIMIT 10\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WJBKCYQZwks"
      },
      "source": [
        "%spark -o test_3_4_2_sdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghKZ3HHOaXIq"
      },
      "source": [
        "grader.grade(test_case_id = 'fastidious', answer = test_3_4_2_sdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne5NaT-6CLns"
      },
      "source": [
        "#### Step 3.4.3: The Momentus Months\n",
        "\n",
        "The data in `filter_1_stocks_sdf` gives closing prices on a daily basis. Since we are interested in monthly trends, we will only keep the closing price on the **last trading day of each month**.\n",
        "\n",
        "Create an sdf `filter_2_stocks_sdf` that contains only the closing prices for the last trading day of each month. Note that a trading day is not simply the last day of each month, as this could be on a weekend when the market is closed . The format of the sdf is shown below:\n",
        "\n",
        "```\n",
        "+----+------------+--------------+\n",
        "|org |date        |Close         |\n",
        "+----+------------+--------------+\n",
        "|IBM |2000-01-31  |...           |\n",
        "|... |...         |...           |\n",
        "+----+------------+--------------+\n",
        "```\n",
        "\n",
        "  _Hint_: This is a **difficult** question. But if you made it this far, you're a star by now. It may be helpful to create an intermediate dataframe that will help you filter out the specific dates you desire."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIx5LUuDD4q_"
      },
      "source": [
        "%%spark\n",
        "\n",
        "# TODO: Create [filter_2_stocks_sdf]\n",
        "\n",
        "# Create sdf that has for each company, the closing day for each month. We need\n",
        "# to preform a GROUP BY on three features, org, YEAR(date), and MONTH(date).\n",
        "# This will give us aggregations of the closing stock price for every day of a\n",
        "# specified month and a specified year. Since these are all datetime objects,\n",
        "# taking MAX() will give us the highest, i.e. last, one.\n",
        "\n",
        "\n",
        "\n",
        "# Save as a temporary sdf, desired_months\n",
        "\n",
        "\n",
        "\n",
        "# Merge desired_months with filter_1_stocks. This will allow us to keep the\n",
        "# closing prices for only those dates that were the closing date for a given\n",
        "# month.\n",
        "\n",
        "\n",
        "# Define and save filter_2_stocks_sdf\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM9tfdKpt8Yt"
      },
      "source": [
        "%%spark\n",
        "\n",
        "## AUTOGRADER Step 3.4.3: ##\n",
        "\n",
        "filter_2_stocks_sdf.createOrReplaceTempView(\"test_3_4_3\")\n",
        "test_3_4_3_sdf = spark.sql(\"SELECT org, DATE_FORMAT(date, 'yyyy-MM-dd') as date, Close FROM test_3_4_3 ORDER BY org, date LIMIT 10\")\n",
        "to_submit = pd.read_json(test_3_4_3_sdf.toPandas().to_json())\n",
        "to_submit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1gWwsFsa-PN"
      },
      "source": [
        "%%spark\n",
        "\n",
        "grader.grade(test_case_id = 'momentus', answer = to_submit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG4bACKKEQNl"
      },
      "source": [
        "#### Step 3.4.4: The Really Random Reshape\n",
        "\n",
        "Now, we will begin to shape our dataframe into the format of the final training sdf.\n",
        "\n",
        "Create an sdf `filter_3_stocks_sdf` that has for a single company and a single year, the closing stock price for the last trading day of each month in that year. This is similar to the table you created in Step 3.1. In this case since we cannot make a proxy for the closing price if the data is not avaliable, drop any rows containing any `null` values, in any column. The format of the sdf is shown below:\n",
        "\n",
        "```\n",
        "+----+-----+----------+---------+----------+\n",
        "|org |year |jan_stock |   ...   |dec_stock |\n",
        "+----+-----+----------+---------+----------+\n",
        "|IBM |2008 |...       |   ...   |...       |\n",
        "|IBM |2009 |...       |   ...   |...       |\n",
        "|... |...  |...       |   ...   |...       |\n",
        "+----+-----+----------+---------+----------+\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AucLEgvwIr_0"
      },
      "source": [
        "%%spark\n",
        "\n",
        "# TODO: Create [filter_3_stocks_sdf]\n",
        "\n",
        "# We will do the same operation we did in Step 3.1. In this case, however, as\n",
        "# the question specifies, any missing entry in a given month are set to null.\n",
        "\n",
        "\n",
        "\n",
        "# Define and save filter_3_stocks_sdf\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsrpmUk0uKhe"
      },
      "source": [
        "%%spark\n",
        "\n",
        "## AUTOGRADER Step 3.4.4: ##\n",
        "\n",
        "filter_3_stocks_sdf.createOrReplaceTempView(\"test_3_4_4\")\n",
        "test_3_4_4_sdf = spark.sql(\"SELECT * FROM test_3_4_4 ORDER BY org, year LIMIT 12\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udBH2pDTZ9W_"
      },
      "source": [
        "%spark -o test_3_4_4_sdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxYnOS14bqZH"
      },
      "source": [
        "grader.grade(test_case_id = 'random', answer = test_3_4_4_sdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82OQKp-nIulq"
      },
      "source": [
        "#### Step 3.4.5: The Decisive Direction\n",
        "\n",
        "The final element in our training set is the binary output for each case, i.e. the `y` label. \n",
        "\n",
        "Create an sdf `stocks_train_sdf` from `filter_3_stocks_sdf` with an additional column `direction`. This should be the direction of percentage change in the closing stock price, i.e. `1` for positive or `-1` for negative, in the first quarter of a given year. Make this an **integer**.  The quarter of a year begins in January and ends in April, inclusive. We want to know the percent change between these two months. Reference Step 2.2 for the percent change formula. The format of the sdf is shown below:\n",
        "\n",
        "```\n",
        "+----+-----+----------+---------+----------+-------------+\n",
        "|org |year |jan_stock |   ...   |dec_stock |direction    |\n",
        "+----+-----+----------+---------+----------+-------------+\n",
        "|IBM |2008 |...       |   ...   |...       |1            |\n",
        "|IBM |2009 |...       |   ...   |...       |-1           |\n",
        "|... |...  |...       |   ...   |...       |...          |\n",
        "+----+-----+----------+---------+----------+-------------+\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEFJIfyZKf7B"
      },
      "source": [
        "%%spark\n",
        "\n",
        "# TODO: Create [stocks_train_sdf]\n",
        "\n",
        "# SIGN() will return -1 if the input is negative, 0 if the input is zero, and 1\n",
        "# if the input is positive.\n",
        "\n",
        "# Keep all rows in filter_3_stocks and add another based on the sign of the\n",
        "# percentage change in stock\n",
        "\n",
        "\n",
        "\n",
        "# Define and save stocks_train_sdf\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thJtUQY6uwYA"
      },
      "source": [
        "%%spark\n",
        "\n",
        "## AUTOGRADER Step 3.4.5: ##\n",
        "\n",
        "stocks_train_sdf.createOrReplaceTempView(\"test_3_4_5\")\n",
        "test_3_4_5_sdf = spark.sql(\"SELECT * FROM test_3_4_5 ORDER BY org, year LIMIT 10\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5QX-H18aU3W"
      },
      "source": [
        "%spark -o test_3_4_5_sdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIR72BANd9Mv"
      },
      "source": [
        "grader.grade(test_case_id = 'decisive', answer = test_3_4_5_sdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd2nviNpM2dF"
      },
      "source": [
        "### Step 3.5: The C-r-a-z-y Combination\n",
        "\n",
        "Now that we have individually created the two halves of our training data we will merge them together to create the final training sdf we showed in the beginning of Step 3.\n",
        "\n",
        "Create an sdf called `training_sdf` in the format of the one shown at the beginning of Step 3. Note that in our definition for the `stock_result` column, the `stock_result` value for a particular year corresponds to the direction of the stock percentage change in the **following** year. For example, the stock_result in the `2008` row for `IBM` will contain the direction of IBM's stock in the first quarter of 2009. The format of the sdf is shown below:\n",
        "```\n",
        "+----+-----+----------+---------+----------+----------+---------+----------+-------------+\n",
        "|org |year |jan_hired |   ...   |dec_hired |jan_stock |   ...   |dec_stock |stock_result |\n",
        "+----+-----+----------+---------+----------+----------+---------+----------+-------------+\n",
        "|IBM |2008 |...       |   ...   |...       |...       |   ...   |...       |-1           |\n",
        "|IBM |2009 |...       |   ...   |...       |...       |   ...   |...       |1            |\n",
        "|... |...  |...       |   ...   |...       |...       |   ...   |...       |...          |\n",
        "+----+-----+----------+---------+----------+----------+---------+----------+-------------+\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZIb6QkcO5RB"
      },
      "source": [
        "%%spark\n",
        "\n",
        "# TODO: Create [training_sdf]\n",
        "\n",
        "# Our merge will consist of two joins. The first will use filter_3_stocks to\n",
        "# join the monthly hiring rates and closing prices. The next join will be with\n",
        "# stock_train and to find stock_result. This join will be done such that the\n",
        "# correct years are matched between hire_train and stocks_train (think about how \n",
        "# to get one year's stock result to be the direction of the stock for the following\n",
        "# year's first quarter). \n",
        "\n",
        "\n",
        "\n",
        "# Define and save training_sdf\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snJwWRAXtB-7"
      },
      "source": [
        "%%spark\n",
        "\n",
        "## AUTOGRADER Step 3.5: ##\n",
        "\n",
        "training_sdf.createOrReplaceTempView(\"test_3_5\")\n",
        "test_3_5_sdf = spark.sql(\"SELECT * FROM test_3_5 ORDER BY org, year LIMIT 10\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nqS07o6adR3"
      },
      "source": [
        "%spark -o test_3_5_sdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpcWi4pYta1m"
      },
      "source": [
        "sum(test_3_5_sdf['nov_stock'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMMK9rAQekCh"
      },
      "source": [
        "grader.grade(test_case_id = 'crazy', answer = test_3_5_sdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnwHS5-kPtVl"
      },
      "source": [
        "## Step 4: Machine ... Learning?\n",
        "\n",
        "Well here we go. Who's ready to make some money? Well... it's not gonna happen. We didn't code the random forest model, sorry! The second half of the course will be about scalable machine learning, and we will learn how to take this beautiful data and make billions of dollars.\n",
        "\n",
        "![Jumping for $$](https://cdn.dribbble.com/users/2749602/screenshots/7065140/shot-cropped-1567085403260.png)\n",
        "\n",
        "One last thing, as I predicted before, you're a star.\n",
        "\n",
        "Feel free to fill out [this form](https://forms.gle/DbDuEbqqifoFrRxaA) with any feedback for this and prior homeworks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5KO8ba2zdxf"
      },
      "source": [
        "## Optional Extra-Credit Step: Full PageRank on Spark (5 points)\n",
        "\n",
        "We've given a basic implementation of MapReduce using Spark's matrix types in:\n",
        "https://colab.research.google.com/drive/1Mr2zf-Oz6W9kRFrzSN08S1lo14mGYnw2\n",
        "\n",
        "Your task for extra credit, worth up to 5 points, is to take this basic example and flesh it out:\n",
        "\n",
        "1. You should write a function called `pagerank` that takes two inputs: (1) a Spark dataframe `df` conforming to the schema of `initial_graph`, (2) an integer `n` specifying the max number of iterations until termination.  It **returns a Spark dataframe** with two columns: `node_id` and `pagerank` (the latter can be of type double), where the latter is the PageRank score after $n$ iterations.\n",
        "\n",
        "2. Your PageRank algorithm should incorporate the standard \"decay factor\" as we've described in the lecture slides.  Use the standard value $\\alpha=0.85$.  It should use Apache Spark matrices to do the computation.\n",
        "\n",
        "3. Your PageRank algorithm should remove sinks and self-loops."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRMdl7kd58H0"
      },
      "source": [
        "# EXTRA CREDIT HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHVFHPVIrvu1"
      },
      "source": [
        "# HW Submission\n",
        "\n",
        "Before you submit on Gradescope (you must submit your notebook to receive credit):\n",
        "\n",
        "\n",
        "1.   Restart and Run-All to make sure there's nothing wrong with your notebook\n",
        "2.   **Double check that you have the correct PennID (all numbers) in the autograder**. \n",
        "3. Make sure you've run all the PennGrader cells\n",
        "4. Go to the \"File\" tab at the top left, and click \"Download .ipynb\" and then \"Download .py\".  **Rename** the files to \"homework3.ipynb\" and \"homework3.py\" respectively and upload them to Gradescope **through the Coursera LTI item.**\n",
        "\n",
        "**Let the course staff know ASAP if you have any issues submitting, but otherwise best of luck!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNNg19-HZqKF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}