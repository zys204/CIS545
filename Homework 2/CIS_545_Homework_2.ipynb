{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIS 545 Homework 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlv8esDGMj0h"
      },
      "source": [
        "# CIS 545 Homework 2\n",
        "### Worth 100 points in total\n",
        "\n",
        "Welcome to Homework 2! By now, you should be familiar with the world of data science and the Pandas library. This assignment will focus on broadening both of these horizons by covering hierarchical data, graphs, and traversing relationships as well as two new tools: SQL and Spark. \n",
        "\n",
        "In the first section of the homework, we will familiarize ourselves with SQL (specifically **pandasql**, which lets you use SQL over dataframes, and explore the Stack Exchange dataset, which is for a set of sites including Stack Overflow). We will also finish out the section with some text analysis.\n",
        "\n",
        "The second section will focus on graph data and give you a small preview of Apache Spark using the Yelp dataset. This homework is designed to introduce you to Spark's required workflow before you fully unleash its power next homework and deploy it on an AWS cluster.  You'll also get comfortable with SparkSQL.\n",
        "\n",
        "We are introducing a lot of new things in this homework, and it is often where students start to get lost in the data science sauce, so we **strongly** encourage you to review the slides/material as you work through this assignment and will try to link the most relevant sections!\n",
        "\n",
        "You may also want to refer to:\n",
        "* [PandaSQL](https://pythonrepo.com/repo/yhat-pandasql-python-data-containers)\n",
        "* [Spark SQL Reference](https://spark.apache.org/docs/latest/sql-ref.html)\n",
        "\n",
        "**Before you Begin**\n",
        "- **Be sure to click \"Copy to Drive\" to make sure you are working on your own personal version of the homework**\n",
        "- Read the Piazza and FAQ for updates! If you have been stuck, chances are other students are too! We don't want you to waste away for two hours trying to get that last point on the autograder so do check Piazza for similar struggles or even homework bugs that will be clarified in the FAQ :) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XoSSg8VDX_4"
      },
      "source": [
        "# Section 0: Homework Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeyWoMn6pxSC"
      },
      "source": [
        "## Part -1: Enter your PennID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ds2HHSkpvBO"
      },
      "source": [
        "STUDENT_ID =  #ENTER YOUR PENNID HERE\n",
        "print(type(STUDENT_ID) == int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0hcZWDcqCUL"
      },
      "source": [
        "## Part 0: Libraries and Set Up Jargon (The usual wall of imports)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1C8EIvyEq1S"
      },
      "source": [
        "!pip3 install penngrader\n",
        "\n",
        "from penngrader.grader import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTE9TG8Aqaz8"
      },
      "source": [
        "grader = PennGrader(homework_id = 'CIS545_MCITO_Fall_2021_HW2', student_id = STUDENT_ID)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiOqbuZCF-EL"
      },
      "source": [
        "### Install required packages\n",
        "%%capture\n",
        "!pip3 install lxml\n",
        "!pip install pandasql"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zkXvBH-F-l4"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "\n",
        "import gc\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "import json # JSON parsing\n",
        "from lxml import etree # HTML parsing\n",
        "import time # Time conversions\n",
        "from lxml import etree # XML Parser\n",
        "import pandasql as ps #SQL on Pandas Dataframe\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzZ6JNnKLplQ"
      },
      "source": [
        "\n",
        "\n",
        "# Section 1: Exploring the Stack Exchange Dataset\n",
        "\n",
        "\n",
        "<img src = \"https://cdn.sstatic.net/Sites/stackoverflow/company/img/logos/se/se-logo.png?v=dd7153fcc7fa\" width= \"600\" align =\"center\"/>\n",
        "\n",
        "To survive as a student at Penn , you've certainly used Stack Exchange or Stack Overflow, as a source for all your technical queries. Stack Exchange looks a lot like a social network, it has the following pieces of information to tie it all together:\n",
        "\n",
        "\n",
        "*   `Users`: All stack exchange users including admins etc.\n",
        "\n",
        "*   `Posts`: All the questions as well as the answers that users post\n",
        "\n",
        "*   `Comments`: As the name suggests, these are comments on posts\n",
        "\n",
        "*   `Votes`: Up/Downvotes \n",
        "\n",
        "\n",
        "For this homework we'll be parsing this data (dumped in XML) into dataframes and relations, and then exploring how to query and assemble the tables into results with Pandas and  PandaSQL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oENmSeFkFRCo"
      },
      "source": [
        "## Part 1: Loading our datasets [12 points total]\n",
        "\n",
        "Before we get into the data, we first need to load our datasets. We will actually only be using the Users and Posts datasets for our queries, but we want you to write a generalized xml parsing function that would be able to convert any of the xml files into a dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aAGG11y8I4r"
      },
      "source": [
        "### 1.0 Importing Data\n",
        "\n",
        "Below is the code to download the XML files containing Stack Exchange data from Google Cloud Storage. The data is relatively small, so this shouldn't take too long. We will only import the `Users` and `Posts` XML files for now, but the other datasets are there in case you want to take a look :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm3G_ZtXFpH_"
      },
      "source": [
        "!wget -nc https://storage.googleapis.com/penn-cis545/Users.xml -P /content\n",
        "!wget -nc https://storage.googleapis.com/penn-cis545/Posts.xml -P /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX6NErPmFwfu"
      },
      "source": [
        "### 1.1 Load Dataset Function\n",
        "\n",
        "Now that we finally have all our packages imported and datasets initalized, it's time to finally write some code! Your first task is to write the function **xml_to_df(file_path)** that will parse the specified file into a dataframe. This function should be generalized, in the sense that it can accept any of the xml files that we load and return a dataframe. We highly recommend looking over the [xml documentation](https://docs.python.org/2/library/xml.etree.elementtree.html) in order to accomplish this task.\n",
        "\n",
        "**TODO:** Once you have written **xml_to_df(file_path)**, create a **posts_df** and **users_df** with the parsed XML files (`/content/Users.xml` and `/content/Posts.xml`)\n",
        "\n",
        "Tip: try figuring out the steps with one of the two XML files first! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJx3M62XHS5q"
      },
      "source": [
        "#Solution\n",
        "def xml_to_df(file_path):\n",
        "  \"\"\" Converts an xml file to a dataframe\n",
        "\n",
        "  :param file_path: path to file\n",
        "  :return: dataframe \n",
        "  \"\"\"\n",
        "\n",
        "  ## TODO\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WxkXoLzGHvz"
      },
      "source": [
        "posts_df = ##TODO: call your function here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ7N7VCjoi-i"
      },
      "source": [
        "# [CIS 545 PennGrader Cell] - 5 points\n",
        "grader.grade(test_case_id = 'test_xml_to_posts_df', answer = posts_df[:75])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBV1ZMjzS4ja"
      },
      "source": [
        "users_df = ##TODO: call your function here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmpIsMjcS5wD"
      },
      "source": [
        "grader.grade(test_case_id = 'test_xml_to_users_df', answer = users_df[:75])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPitr7eRJfO0"
      },
      "source": [
        "### 1.2 Clean Dataset\n",
        "\n",
        "Next, we are going to want to clean up our dataframes, namely:\n",
        "\n",
        "1. replacing null values,\n",
        "1. changing datatypes, and \n",
        "1. dropping columns\n",
        "\n",
        "Originally, we were going to have you identify the datatypes with this [image](https://i.stack.imgur.com/AyIkW.png) on your own, but we found this part really tedious and rage-inducing, so we have defined the specific columns to convert below. All you need to do is write the function :)\n",
        "\n",
        "**TODO**: \n",
        "1. replace all null values in both datasets **in-place**. \n",
        "1. define a function **dtype_converter(df, int_columns)** that takes in a dateframe and a list specifying which columns should be integers. \n",
        "1. Then, use **dtype_converter** on both `posts_df` and `users_df` using the lists defined below. (Note: we don't need to convert any columns to strings since they're already objects, and we're ignoring datetime)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlWGmIvB-qtl"
      },
      "source": [
        "##TODO replace NA values here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptWgCcPeGWHt"
      },
      "source": [
        "#columns that need to be integers\n",
        "\n",
        "int_posts_cols = [\"Id\", \"PostTypeId\", \"AcceptedAnswerId\", \"ParentId\", \"Score\", \n",
        "                  \"ViewCount\", \"OwnerUserId\", \"LastEditorUserId\", \"AnswerCount\",\n",
        "                  \"CommentCount\", \"FavoriteCount\"]\n",
        "int_users_cols = [\"Id\", \"Reputation\", \"Views\", \"UpVotes\", \"DownVotes\", \"AccountId\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6uZaW_GJ7_s"
      },
      "source": [
        "def dtype_converter(df, int_columns):\n",
        "  \"\"\"converts columns to type integer\n",
        "\n",
        "  :param df: dataframe to convert\n",
        "  :param int_columns: list of columns to convert\n",
        "  :return: dataframe\n",
        "  \"\"\"\n",
        "  #TODO: implement function below"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ub5pF19jKn_X"
      },
      "source": [
        "posts_df = #TODO: call function here \n",
        "users_df = #TODO: call function here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOsPH8RcWNoP"
      },
      "source": [
        "#check your datatypes\n",
        "posts_df.dtypes "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ngIbo_vVK5a"
      },
      "source": [
        "grader.grade(test_case_id = 'test_posts_dtypes', answer = posts_df[:75])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6WJkrBGVHGg"
      },
      "source": [
        "grader.grade(test_case_id = 'test_users_dtypes', answer = users_df[:75])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYp9fW_SvG3g"
      },
      "source": [
        "## Part 1.5 Your Sandbox \n",
        "\n",
        "Instead of throwing you straight into the deep end, we wanted to give you a chance to take some time and explore the data on your own. **This section is not graded**, so for the speedrunners out there feel free to just jump in, but we wanted to at least give you a small space to utilize your basic EDA toolkit to familiarize yourself with all the info you just downloaded.\n",
        "\n",
        "Some suggestions to get you started:\n",
        "- `df.head()`\n",
        "- `df.info()`\n",
        "- `df.describe()`\n",
        "\n",
        "Also, definitely take a look at [this readme](https://ia800107.us.archive.org/27/items/stackexchange/readme.txt) that provides a good overview of all the datasets (ignore the ones that you did not ask you to convert)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYbq6dN5snWs"
      },
      "source": [
        "# your EDA here! feel free to add more cells"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHFdRtQKLbti"
      },
      "source": [
        "## Part 2: Exploring the data with Pandas and PandasSQL [20 points total]\n",
        "\n",
        "Now that you are familiar (or still unfamiliar) with the dataset, we will now introduce you to SQL, or more specifically **pandasql**: a package create to allow users to query pandas DataFrames with SQL statements.\n",
        "\n",
        "The typical flow to use pandasql (shortened to **ps**) is as follows:\n",
        "1. write a SQL query in the form of a string (Tip: use triple quotes \"\"\"x\"\"\" to write multi-line strings)\n",
        "2. run the query using **ps.sqldf(your_query, locals())**\n",
        "\n",
        "Pandasql is convenient in that it allows you to reference the dataframes that are currently defined in your notebook, so you will be able to fully utilize the `posts_df` and `users_df` that you have created above!\n",
        "\n",
        "Given that it is a brand new language, we wanted to give you a chance to directly compare the similarities/differences of the pandas that you already know and the SQL you are about to learn. Thus, for each query, we ask that you to **look into the question twice: once with pandas and once with pandasql**. \n",
        "\n",
        "Each answer will thus require both a `pd_` and `sql_` prefixed-dataframe that you will submit seperately to the autograder. **We will be reviewing your code to make sure you wrote the code in the corresponding languages.**\n",
        "\n",
        " [Here](https://community.alteryx.com/t5/Data-Science/pandasql-Make-python-speak-SQL/ba-p/138435) is a good resource to review pandasql. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geYyH57csade"
      },
      "source": [
        "### 2.1 Splitting Up `posts_df`\n",
        "\n",
        "`posts_df` actually contains both posted questions and the answers. The provided readme details the distinguishing factors as follows:\n",
        "\n",
        "        - PostTypeId\n",
        "            - 1: Question\n",
        "            - 2: Answer\n",
        "        - ParentID (only present if PostTypeId is 2)\n",
        "        - AcceptedAnswerId (only present if PostTypeId is 1)\n",
        "\n",
        "**TODO:** Using pandas/pandasql, split `posts_df` into a `pd/sql_questions_df` and `pd/sql_answers_df` based on these values of `PostTypeId`. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laL9vimDbPYQ"
      },
      "source": [
        "pd_questions_df = #TODO\n",
        "pd_answers_df = #TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqQjSBREeNQL"
      },
      "source": [
        "grader.grade(test_case_id = 'test_pd_questions_df', answer = pd_questions_df[\"PostTypeId\"].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tGZjFP-gdVo"
      },
      "source": [
        "grader.grade(test_case_id = 'test_pd_answers_df', answer = pd_answers_df[\"PostTypeId\"].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOIT18_uqJ8c"
      },
      "source": [
        "questions_query = #TODO\n",
        "answers_query = #TODO \n",
        "\n",
        "sql_questions_df = #TODO use ps.sqldf here on questions_query\n",
        "sql_answers_df = #TODO use ps.sqldf here on answers_query"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKqtjXDPeNjd"
      },
      "source": [
        "grader.grade(test_case_id = 'test_qa_query', answer = (questions_query,answers_query))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r_DeMOYjldH"
      },
      "source": [
        "#using just our sql dataframe moving forward\n",
        "questions_df = sql_questions_df\n",
        "answers_df = sql_answers_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooVqjTGm9eCA"
      },
      "source": [
        "### 2.2 What are the most popular questions?\n",
        "\n",
        "**TODO**: Use `questions_df` to find the 10 most popular questions by `ViewCount`.\n",
        "\n",
        "Store the results in `pd/sql_popular_df` which be have the following format:\n",
        "\n",
        ">Id | Title | ViewCount\n",
        ">--- | --- | ---\n",
        "\n",
        "Hint: for your SQL query, you will need to know `ORDER BY`, `LIMIT`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scVrGa4CjS08"
      },
      "source": [
        "pd_popular_df = #TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_hw6BLSj3-p"
      },
      "source": [
        "grader.grade(test_case_id = 'test_pd_popular_df', answer = pd_popular_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK2IfiYa-FKL"
      },
      "source": [
        "popular_query = #TODO\n",
        "sql_popular_df = #TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHzgw1okj7Ju"
      },
      "source": [
        "grader.grade(test_case_id = 'test_popular_query', answer = popular_query)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcJvhlRfkTgt"
      },
      "source": [
        "grader.grade(test_case_id = 'test_sql_popular_df', answer = sql_popular_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE5L0XqjBLu_"
      },
      "source": [
        "### 2.3 Who are the most helpful users?\n",
        "\n",
        "**TODO:**  Use `answers_df` to find the names of the top 10 users who answer the most questions on stack exchange. This should be based on the count of unique answers made by the user.\n",
        "\n",
        "Your answer, stored in `pd/sql_talkative_df` will have the following format:\n",
        "\n",
        ">UserId | DisplayName | ResponseCount\n",
        ">--- | --- | ---\n",
        "\n",
        "\n",
        "Note: both `users_df` and `answers_df` have an `Id` column, but store entirely different values in them! \n",
        "\n",
        "SQL Hint: The tools that you will need include, but are not limited to`AS`, `JOIN`, `GROUP BY`, `ORDER BY` and `LIMIT`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lC-kw648e_d"
      },
      "source": [
        "pd_talkative_df = #TODO\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xZO_F9D06E8"
      },
      "source": [
        "grader.grade(test_case_id = 'test_pd_talkative_df', answer = pd_talkative_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpQevBFf9DUL"
      },
      "source": [
        "#theres probably going to be a lot of questions on how to order by count here\n",
        "talkative_query = #TODO\n",
        "sql_talkative_df = #TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhGqClcm1KRO"
      },
      "source": [
        "grader.grade(test_case_id = 'test_talkative_query', answer = talkative_query)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oidAVJro2Eda"
      },
      "source": [
        "grader.grade(test_case_id = 'test_sql_talkative_df', answer = sql_talkative_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdwzQ2XtF2ki"
      },
      "source": [
        "### 2.4 Who are the most helpful-in-a-different-kind-of-way users?\n",
        "\n",
        "**TODO**: find the users that ask a lot of questions, but have never posted an answer. To accomplish this, you are going to want to find all the users in `questions_df` that don't appear in `answers_df`. Sort by `QuestionsCount` descending and store only the top 5 results.\n",
        "\n",
        "The query will require you to write a [nested SQL query](https://learnsql.com/blog/sql-nested-select/). That is, there will be at least one select statement inside of a select statement. This means that you **should NOT** write two seperate SQL commands and call ps.sqldf() twice. \n",
        "\n",
        "Though it would be helpful, **you do NOT have to implement this in pandas**. Your answer, stored in `askers_df` will have the following format:\n",
        "\n",
        ">UserId | DisplayName | QuestionsCount\n",
        ">--- | --- | ---\n",
        "\n",
        "\n",
        "SQL Hint: You can use `NOT IN` or `LEFT JOIN`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_zlC7N5alOr"
      },
      "source": [
        "askers_query = #TODO\n",
        "askers_df = #TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et3CfB-VVrGR"
      },
      "source": [
        "grader.grade(test_case_id = 'test_askers_query', answer = askers_query)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S96GfLS8Vzn9"
      },
      "source": [
        "grader.grade(test_case_id = 'test_askers_df', answer = askers_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmzgmtU2USvQ"
      },
      "source": [
        "### 2.5 So which is better, SQL or Pandas?\n",
        "\n",
        "Now that you have a taste for SQL, let's try to use our new skill to query stack exchange in this notebook and put this debate to rest.\n",
        "\n",
        "**TODO**: Find all of the answers to a post that asks about Pandas vs. SQL. Here are some clues that will come in handy:\n",
        "1. This post contains the words \"pandas\" and \"sql\"\n",
        "2. This post has the most viewcount out of all the posts with both of those words\n",
        "3. The answers to this post have the column `ParentId` equal to the post's `Id`\n",
        "\n",
        "Again, no need to do this in pandas, but your answer, stored in `versus_df` will have the following format:\n",
        "\n",
        "> QuestionId | Question | QuestionBody | AnswerId | AnswerBody \n",
        ">--- | --- | --- | --- | ---\n",
        "\n",
        "SQL Hint: take a look at the `LIKE` function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ONi-nKJa2sp"
      },
      "source": [
        "versus_query = #TODO\n",
        "versus_df = #TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18GhQqQkR-YC"
      },
      "source": [
        "We highly recommend that you read the responses! They are actually all pretty accurate and go into the pros/cons that you probably encountered while working through the problem set. Use `pd.set_option('display.max_colwidth', -1)` to view the full columns and when you're done set the colwidth back to a value like `20` so that you don't have giant dataframes in the next steps.\n",
        "\n",
        "(You could also try to find the same question via Google Search)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7p-bUUjDR9u9"
      },
      "source": [
        "#pd.set_option('display.max_colwidth', -1)\n",
        "#versus_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2VBUPWLd5J0"
      },
      "source": [
        "grader.grade(test_case_id = 'test_versus_query', answer = versus_query)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXxWaEdOd7ZJ"
      },
      "source": [
        "grader.grade(test_case_id = 'test_versus_df', answer = versus_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbNUhlz9ftHI"
      },
      "source": [
        "## Part 3: Working with Text Data [22 points]\n",
        "\n",
        "Shifting gears, let's now try to do some text-based analysis. Our Stack Exchange data has plenty of text that we can play with, from the user descriptions to the posts themselves. Text data is complex, but can also be used to generate extremely interpretable results, making it valuable and interesting. \n",
        "\n",
        "Throughout this section, we will attempt to answer the following:\n",
        "\n",
        "### What types of questions should I ask to get a higher reputation on Stack Exchange? \n",
        "\n",
        "Users on stack exchange are valued based on their reputation, which depends on the quality of your posts. Each post receives a score, where **score = number of upvotes - number of downvotes**. This value is already present in your posts_df. \n",
        "\n",
        "Both questions and answers get scores, but let's just focus on what types of questions we should/shouldn't ask in order to get a higher score and thus higher reputation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhKJNx74fLZx"
      },
      "source": [
        "### 3.1 Getting Highest and Lowest Scored Posts\n",
        "**TODO:** First, let's get questions with the negative scores from `questions_df` and then get the **same number** of questions with highest scores. Convert the **Body** column of the highest/lowest scorers into two lists: **highest_content** and **lowest_content**. Be sure to sort when needed!\n",
        "\n",
        "Feel free to use either **pandas** or **pandasql** to accomplish this :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2Wp8Wh_oSZP"
      },
      "source": [
        "highest_content = #TODO: should be a list\n",
        "lowest_content = #TODO: should be a list "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_ZJHjNKfMTu"
      },
      "source": [
        "grader.grade(test_case_id = 'test_lowest_content', answer = lowest_content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWWkJMFXglNf"
      },
      "source": [
        "grader.grade(test_case_id = 'test_highest_content', answer = highest_content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyQXqWu20mkc"
      },
      "source": [
        "### 3.2 Cleaning our Text with Regex\n",
        "Now that we have the content of our highest/lowest scored posts, we will now need to clean and tokenize them. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cCz0zh2ZSzM"
      },
      "source": [
        "First, before we do anything, let's just take a look at what we are working with\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5KJ4fYy2v9d"
      },
      "source": [
        "highest_content[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wduz_X6a25lN"
      },
      "source": [
        "You probably noticed a couple of things:\n",
        "\n",
        "1. html tags (\\<p\\>, \\<a\\>, etc.)\n",
        "2. embedded latex (words surrounded $$)\n",
        "3. newline characters(\\n)\n",
        "\n",
        "We are going to clean out all of these cases using **regex**, a staple text processing tool that matches strings based on a specified pattern. Creating these patterns is actually considered a form of art to some, as the syntax is very extensive. As a brief introduction here are some basic pattern components that you will need to know:\n",
        "- `c`: matches a \"c\" character in a string\n",
        "- `c*?`: matches 0 or more c characters\n",
        "- `.` matches any character\n",
        "- `.*?c`: matches any characters until you encounter \"c\"\n",
        "\n",
        "Note: the `?` makes the asterisks less greedy and severe when removing parts of the string. It's good practice to include it, but not always necessary.\n",
        "\n",
        "**TODO:** Below, create a function **remove_bad_patterns(text)** that removes all of the 3 cases listed above from a given string, text. You will need to \n",
        "\n",
        "1. create patterns to handle each of the cases\n",
        "1. use **re.sub(pattern, newstring)** to substitute all matches with the empty string, `\"\"`. If you want to test your pattern, check out [this tool](https://regexr.com).\n",
        "\n",
        "Note: `$` is considered a special character in regex, so you will need to escape it with `\\\\$` to specify you want to match the character.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RddxrRuFsWoh"
      },
      "source": [
        "import re\n",
        "\n",
        "def remove_bad_patterns(text):\n",
        "    \"\"\"Remove html, latex, and newline characters from a string\n",
        "    \n",
        "    :param text: content as a string\n",
        "    :return: cleaned text string\n",
        "    \"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KuHZkyfMjHB"
      },
      "source": [
        "Now, apply this function to both **highest_content** and **lowest_content** to create **cleaned_highest_content** and **cleaned_lowest_content**, respectively, and let's take another look at the new and improved first entry:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHv_uU6-L5kL"
      },
      "source": [
        "cleaned_highest_content = #TODO\n",
        "cleaned_lowest_content =  #TODO\n",
        "cleaned_highest_content[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7VMJsGZh9Ym"
      },
      "source": [
        "grader.grade(test_case_id = 'test_cleaned_highest', answer = cleaned_highest_content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oqa35Zyhh9jH"
      },
      "source": [
        "grader.grade(test_case_id = 'test_cleaned_lowest', answer = cleaned_lowest_content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK0qh7TRMoj7"
      },
      "source": [
        "A lot cleaner, right? Of course, it's not perfect but it'll do for our purposes in this homework. With that out of the way let us now...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNwLCBeTOLy3"
      },
      "source": [
        "###3.3 Tokenize the Text\n",
        "\n",
        "Here, we are going to split up the content into a list of words. Here, we will use the **nltk** package, which contains an extensive set of tools to process text. Of course, like regex, this homework would be miles long if we really went into detail, so we are only going to utilize the following components:\n",
        "- **nltk.word_tokenize()**: a function used to tokenize our text\n",
        "- **nltk.corpus.stopwords**: a list of commonly used words such as `a`, `an`, `in` that are often ignored in text-related analysis\n",
        "\n",
        "\n",
        "**TODO:** First, use **stopwords** to create a set of the most common english stopwords. Then, implement **tokenized_content(content)** that takes in a content string and \n",
        "1. tokenizes the text\n",
        "2. lowercases the token\n",
        "3. removes stop words (commonly used words such as \"a\",\"an\", \"in\")]\n",
        "4. keeps words with only alphabet characters (no punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIWiVzUUpvjA"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEwEm-E7pxjT"
      },
      "source": [
        "stopwords = set(stopwords.words('english')) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gbi4KvlXpJXV"
      },
      "source": [
        "def tokenize_content(content):\n",
        "  \"\"\"returns tokenized string\n",
        "\n",
        "  :param content: text string\n",
        "  :return: tokenized text/list of words\n",
        "  \"\"\"\n",
        "  #TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Es5zPleHSROw"
      },
      "source": [
        "Now, apply your tokenized_titles function to each piece of content in **cleaned_highest_content** and **cleaned_lowest content** and flatten both of the lists to create **highest_tokens** and **lowest_tokens**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwsxzxdYpiic"
      },
      "source": [
        "highest_tokens = #TODO\n",
        "lowest_tokens =  #TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4v6dzmXk6Yp"
      },
      "source": [
        "grader.grade(test_case_id = 'test_highest_tokens', answer = highest_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kPhea-Tk9Nh"
      },
      "source": [
        "grader.grade(test_case_id = 'test_lowest_tokens', answer = lowest_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUYScz9YSKsM"
      },
      "source": [
        "### 3.4 Most Frequent Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT0sTQHvp5E6"
      },
      "source": [
        "Now, find the 20 most common words amongst the content of your highest and lowest questions.\n",
        "\n",
        "\n",
        "\n",
        "Hint: https://docs.python.org/3/library/collections.html#counter-objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRWGYkoep-I3"
      },
      "source": [
        "lowest_counter = #TODO\n",
        "lowest_most_common = #TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vusua1Edp8mm"
      },
      "source": [
        "highest_counter = #TODO\n",
        "highest_most_common = #TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7WeWoRZtEOT"
      },
      "source": [
        "grader.grade(test_case_id = 'test_highest_most_common', answer = highest_most_common)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG2W2YIbuJQj"
      },
      "source": [
        "grader.grade(test_case_id = 'test_lowest_most_common', answer = lowest_most_common)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvwIdmhNZmhY"
      },
      "source": [
        "###3.5 Refining our Lists\n",
        "\n",
        "Hmmm...both of these lists seem to overrepresent the common jargon of data science. Let's try to tease out words that distinguish the high from the low scoring posts. \n",
        "\n",
        "One approach would be to find words in one list that are not in the other. This, however, may be too naive, as even if a word is extremely common in our high list, if it appears only once in our low list, it would get removed from consideration.\n",
        "\n",
        "Let's instead find the difference between the counts within our two lists. With this method, if a word is really common in one, but not the other, the count would only decrease slightly. Alternatively, if a word is common in both lists, it would effectively zero out.\n",
        "\n",
        "**TODO:** Using the difference method, create a **distinct_highest_common** and **distinct_lowest_commonr**  that find the top 20 counts of words within each group of posts after using the difference method described above. Be careful on which list you are subtracting!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vnx8MsbBA99U"
      },
      "source": [
        "distinct_highest_common = #TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT3hzB_vA2UA"
      },
      "source": [
        "distinct_lowest_common = #TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJZutqmMwLoR"
      },
      "source": [
        "grader.grade(test_case_id = 'test_distinct_highest_common', answer = distinct_highest_common)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCCFeH6-wMQS"
      },
      "source": [
        "grader.grade(test_case_id = 'test_distinct_lowest_common', answer = distinct_lowest_common)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztARzQHyexXQ"
      },
      "source": [
        "The lists are much more different right? It seems as if low scoring posts tend to ask a lot about errors/code while higher posts are much more conceptual based.\n",
        "\n",
        "So if you're a looking for a high reputation, don't ask people to debug your code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEAi4a6OcjE4"
      },
      "source": [
        "### 3.6 Word Clouds\n",
        "\n",
        "Before we move on from this dataset, let's do one final step and visualize our results with wordclouds.\n",
        "\n",
        "**TODO**: Take a look at [this documentation](https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html) and create two word clouds for our two groups of distinct tokens.\n",
        "\n",
        "Be sure to create these on the full list of distinct tokens, and not just the top 20. We will be going through your notebooks and manually grading your world clouds (worth 4 points). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYNzQJDicpLw"
      },
      "source": [
        "highest_wordcloud = #TODO\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONjr3Kq1ciCu"
      },
      "source": [
        "lowest_wordcloud = #TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISe_XMH7ivEa"
      },
      "source": [
        "#Section 2: Spark, Hierarchical Data and Graph Data on Yelp Reviews Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJJHiaJ_1P9G"
      },
      "source": [
        "## Getting Started with Apache Spark\n",
        "\n",
        "Now that you've seen how to run SQL queries through pandas, we'll working with running SQL in Apache Spark! Apache Spark is a complex, cluster-based data processing system written in Scala used for big data processing. For the most part, Spark interfaces “smoothly” to Python.\n",
        "\n",
        "While Spark dataframes try to emulate the same programming style as Pandas DataFrames, there are some differences in how you express things. Please refer to the Lecture Slides or the following resources to learn about these differences:\n",
        "\n",
        "* https://ogirardot.wordpress.com/2015/07/31/from-pandas-to-apache-sparks-dataframe/ \n",
        "\n",
        "For this assignment, we are going to get familiar with Spark without worrying too much about sharding and distribution. This isn’t really using it to its strengths -- and in fact you might find Spark to be slow -- but it will get you comfortable with programming in Spark without worrying about distributed nodes, clusters, and spending real dollars on the cloud. For Homework 3, we’ll connect your Jupyter instance to Spark running on the cloud.\n",
        "\n",
        "### Initializing a Connection to Spark\n",
        "\n",
        "We'll open a connection to Spark as follows. From `SparkSession`, you can load data into Spark DataFrames as well as `RDD`s.\n",
        "\n",
        "Run the following cells to setup this part of the notebook!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8RH4R771X6n"
      },
      "source": [
        "%%capture\n",
        "!apt install libkrb5-dev\n",
        "!wget -nc https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!pip install findspark\n",
        "!pip install sparkmagic\n",
        "!pip install pyspark\n",
        "! pip install pyspark --user\n",
        "! pip install seaborn --user\n",
        "! pip install plotly --user\n",
        "! pip install imageio --user\n",
        "! pip install folium --user"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNctzcXRkexY"
      },
      "source": [
        "%%capture\n",
        "!apt update\n",
        "!apt install gcc python-dev libkrb5-dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP28kxLekWG7"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "import os\n",
        "\n",
        "spark = SparkSession.builder.appName('Graphs-HW2').getOrCreate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gm6aXPq1Ulc"
      },
      "source": [
        "%load_ext sparkmagic.magics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiGROEgu1gfN"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "\n",
        "#misc\n",
        "import gc\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "\n",
        "#graph section\n",
        "import networkx as nx\n",
        "#import heapq  # for getting top n number of things from list,dict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# JSON parsing\n",
        "import json\n",
        "\n",
        "# HTML parsing\n",
        "from lxml import etree\n",
        "import urllib\n",
        "\n",
        "# SQLite RDBMS\n",
        "import sqlite3\n",
        "\n",
        "# Time conversions\n",
        "import time\n",
        "\n",
        "# NoSQL DB\n",
        "from pymongo import MongoClient\n",
        "from pymongo.errors import DuplicateKeyError, OperationFailure\n",
        "\n",
        "import os\n",
        "os.environ['SPARK_HOME'] = \"/content/spark-3.1.2-bin-hadoop3.2\" #change this here\n",
        "import pyspark\n",
        "from pyspark.sql import SQLContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF0xipwC1hme"
      },
      "source": [
        "try:\n",
        "    if(spark == None):\n",
        "        spark = SparkSession.builder.appName('Initial').getOrCreate()\n",
        "        sqlContext=SQLContext(spark)\n",
        "except NameError:\n",
        "    spark = SparkSession.builder.appName('Initial').getOrCreate()\n",
        "    sqlContext=SQLContext(spark)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw-YbXpG1owp"
      },
      "source": [
        "### Download data\n",
        "\n",
        "The following code retrieves the Yelp dataset files from Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuRm7t0it3nF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2257560c-3e56-4aa6-af0b-7b78e8c2ac83"
      },
      "source": [
        "#New Download\n",
        "\n",
        "!wget -nc https://storage.googleapis.com/penn-cis545/yelp_review2.csv -P /content\n",
        "!wget -nc https://storage.googleapis.com/penn-cis545/yelp_business_attributes.csv -P /content\n",
        "!wget -nc https://storage.googleapis.com/penn-cis545/yelp_business.csv -P /content\n",
        "!wget -nc https://storage.googleapis.com/penn-cis545/yelp_checkin.csv -P /content\n",
        "!wget -nc https://storage.googleapis.com/penn-cis545/yelp_user.csv -P /content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-22 12:38:17--  https://storage.googleapis.com/penn-cis545/yelp_review2.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.188.128, 64.233.189.128, 108.177.97.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.188.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3791120545 (3.5G) [application/octet-stream]\n",
            "Saving to: ‘/content/yelp_review2.csv’\n",
            "\n",
            "yelp_review2.csv    100%[===================>]   3.53G  73.7MB/s    in 63s     \n",
            "\n",
            "2021-09-22 12:39:21 (57.8 MB/s) - ‘/content/yelp_review2.csv’ saved [3791120545/3791120545]\n",
            "\n",
            "--2021-09-22 12:39:24--  https://storage.googleapis.com/penn-cis545/yelp_business_attributes.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.189.128, 108.177.97.128, 108.177.125.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.189.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 41377121 (39M) [application/octet-stream]\n",
            "Saving to: ‘/content/yelp_business_attributes.csv’\n",
            "\n",
            "yelp_business_attri 100%[===================>]  39.46M  37.4MB/s    in 1.1s    \n",
            "\n",
            "2021-09-22 12:39:26 (37.4 MB/s) - ‘/content/yelp_business_attributes.csv’ saved [41377121/41377121]\n",
            "\n",
            "--2021-09-22 12:39:26--  https://storage.googleapis.com/penn-cis545/yelp_business.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.189.128, 108.177.97.128, 108.177.125.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.189.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31760674 (30M) [application/octet-stream]\n",
            "Saving to: ‘/content/yelp_business.csv’\n",
            "\n",
            "yelp_business.csv   100%[===================>]  30.29M  20.6MB/s    in 1.5s    \n",
            "\n",
            "2021-09-22 12:39:28 (20.6 MB/s) - ‘/content/yelp_business.csv’ saved [31760674/31760674]\n",
            "\n",
            "--2021-09-22 12:39:28--  https://storage.googleapis.com/penn-cis545/yelp_checkin.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.189.128, 108.177.97.128, 108.177.125.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.189.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 135964892 (130M) [application/octet-stream]\n",
            "Saving to: ‘/content/yelp_checkin.csv’\n",
            "\n",
            "yelp_checkin.csv    100%[===================>] 129.67M  51.1MB/s    in 2.5s    \n",
            "\n",
            "2021-09-22 12:39:31 (51.1 MB/s) - ‘/content/yelp_checkin.csv’ saved [135964892/135964892]\n",
            "\n",
            "--2021-09-22 12:39:31--  https://storage.googleapis.com/penn-cis545/yelp_user.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.189.128, 108.177.97.128, 108.177.125.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.189.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1363176944 (1.3G) [application/vnd.ms-excel]\n",
            "Saving to: ‘/content/yelp_user.csv’\n",
            "\n",
            "yelp_user.csv       100%[===================>]   1.27G  56.2MB/s    in 26s     \n",
            "\n",
            "2021-09-22 12:39:58 (49.6 MB/s) - ‘/content/yelp_user.csv’ saved [1363176944/1363176944]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srYXW3JwvIZi"
      },
      "source": [
        "## Part 4: Working with Spark [21 points total]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tjhGYPK1vmm"
      },
      "source": [
        "### 4.1 Load Our Datasets\n",
        "\n",
        "\n",
        "In this section, we'll be using Spark to look into social data from Yelp. To start, let's read our data into Spark. As an example of how to do this, to load the file `input.txt` into a Spark DataFrame, you can use lines like the following.\n",
        "\n",
        "```\n",
        "# Read lines from the text file\n",
        "input_sdf = spark.read.load('input.txt', format=\"text\")\n",
        "```\n",
        "\n",
        "We’ll use the suffix `_sdf` to represent “Spark DataFrame,” much as we used `_df` to denote a Pandas DataFrame. \n",
        "\n",
        "\n",
        "**TODO:** Load the various files from Yelp. Your datasets should be named `yelp_business_sdf`, `yelp_business_attributes_sdf`, `yelp_check_in_sdf`, `yelp_reviews_sdf`, and `yelp_users_sdf`. Submit the first 75 entries of the yelp_business_sdf, sorted by the \"name\" column in ascending order, to the autograder as a pandas dataframe by using the toPandas() function to convert it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mtQl7Bi1rHe"
      },
      "source": [
        "# TODO: load Yelp datasets\n",
        "\n",
        "yelp_business_sdf = #TODO\n",
        "yelp_business_attributes_sdf = #TODO \n",
        "yelp_check_in_sdf = #TODO \n",
        "yelp_reviews_sdf = #TODO\n",
        "yelp_users_sdf = #TODO \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRjwuThgjdze"
      },
      "source": [
        "yelp_business = yelp_business_sdf.toPandas().sort_values(by = ['name'], ascending = True)[:75]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqHKFiT-12wT"
      },
      "source": [
        "grader.grade(test_case_id = 'check_yelp_load', answer = yelp_business[:75])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvm3tmQ7c6N-"
      },
      "source": [
        "One key difference between using Pandas SQL and Spark SQL is that you'll need to create a view of your data before Spark is able to query it. Note that when using a temporary view as we will be doing, Spark does not persist the data in memory. \n",
        "\n",
        "**TODO:** Put all of your Spark dataframes from the previous section into temporary tables. The syntax is as follows:\n",
        "*yelp_business_sdf.createOrReplaceTempView('yelp_business')*\n",
        "\n",
        "Note that when you're accessing the yelp data within Spark SQL later in the homework you'll want to refer to each table as the name of the view you assigned - for instance \"yelp_business\" not \"yelp_business_sdf\". This distinction is important as this table is only visible to Spark as \"yelp_business\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADFIa6jx16Sm"
      },
      "source": [
        "# TODO: save tables with names such as yelp_business, yelp_users\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-C8wBruelo4"
      },
      "source": [
        "Next, explore the sdf's using the Sandbox area below. Some functions that might be useful are:\n",
        "* show (shows the first few rows of data, similar to head in Pandas)\n",
        "* count (counts number of rows, similar to using len in Pandas)\n",
        "* dtypes (same as in Pandas) \n",
        "* describe (use with show to see summary statistics, similar to just describe on its own in Pandas)\n",
        "\n",
        "You are not limited to these functions and are welcome to use any other ones. The purpose of this exploration is to get a sense of what the data looks like before moving on. Again, **this section is not graded**, but we encourage you to get familiar with the data before diving in.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dc4VrDtToDVB"
      },
      "source": [
        "# your EDA here! feel free to add more cells"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBjwu_4ZbgdG"
      },
      "source": [
        "### 4.2 Simple Analytics on the Data\n",
        "In this section, we will be executing Spark operations on the data given. Beyond simply executing the queries, you may try using .explain() method to see more about the query execution. Also, please read the data description prior to attempting the following questions to understand the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S-K77HvqCJi"
      },
      "source": [
        "#### 4.2.1 Spark and Big Data\n",
        "You may be wondering why we can't just use Pandas SQL for analytics on the yelp data. As the data we're working with gets larger data, performance in Pandas will slow - or the data may even be too large to load into Pandas.\n",
        "\n",
        "For a simple example, let's compare how long the same query takes to run in Pandas SQL and Spark SQL.\n",
        "\n",
        "**TODO:** First, convert the yelp business table to Pandas. Then, using the yelp business table, select the name of businesses located in Pennsylvania. Run this query in both Pandas SQL and Spark SQL and time how long the query takes to run. The time module will be useful for this. You may want to separate your code into several cells to ensure you are only timing one query at a time. Submit the ratio of the time it took the query to run in Pandas SQL to the time it took the query to run in Spark SQL, called `time_ratio`, to the autograder.\n",
        "\n",
        "As a reminder, Spark uses lazy computation, meaning results are delayed until they are actually needed. Therefore, you will need to show your table (or do some other computation that requires the table to be generated) in order for your query to run in Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6i-3fPdTKxH"
      },
      "source": [
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljPlRHFAlR35"
      },
      "source": [
        "# TODO: Convert the yelp_business_sdf to Pandas \n",
        "yelp_business_df = #TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adEbp1mhqKKR"
      },
      "source": [
        "# TODO: Time the query takes to run in Pandas SQL\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Df-qacPRjsny"
      },
      "source": [
        "# TODO: Time the query takes to run in Spark SQL\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vABMmUcwl6xS"
      },
      "source": [
        "# TODO: Ratio of time taken in Pandas SQL to time taken in SPark SQL\n",
        "\n",
        "time_ratio = #TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5xWd4MJr6sr"
      },
      "source": [
        "grader.grade(test_case_id = 'time_check', answer = time_ratio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6n8ooy9iIqH"
      },
      "source": [
        "#### 4.2.2 Cities by number of businesses\n",
        "\n",
        "Next, we'll explore which cities have the most restaurants. \n",
        "\n",
        "**TODO:** Find the top 10 cities by number of (Yelp-listed) businesses. This table should include `city`, `state`, and `num_restaurants`, which is the number of restaurants in the city. Convert this sdf to Pandas and submit top10_cities_df to the autograder. Remember to only convert small tables to Pandas!\n",
        "\n",
        "\n",
        "Your table should look something like:\n",
        ">city | state | num_restaurants\n",
        ">--- | --- |--- \n",
        ">city 1 | state 1|  rating 1 | number 1\n",
        ">city 2| state 2| rating 2 | number 2\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7lPtnowiH5u"
      },
      "source": [
        "# TODO: cities by number of businesses\n",
        "# Worth 5 points\n",
        "\n",
        "top10_cities_df = #TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRDb7UcThGDE"
      },
      "source": [
        "grader.grade(test_case_id = 'top10CitiesCheck', answer = top10_cities_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L8pEamDbeUC"
      },
      "source": [
        "#### 4.2.3 Business ratings across states\n",
        "\n",
        "Next, we'll be looking into how ratings for the same business vary state by state. Throughout this problem, we'll be intersted in the *average rating by business and by state*. \n",
        "\n",
        "**TODO:** For each business, find the states where the business's average rating is below the *maximum of the business's per-state* average rating.  Think about how to factor that into steps!\n",
        "\n",
        "* Compute the average rating for each business name by state. For each business, find the maximum average rating across all states' average ratings. \n",
        "\n",
        "* Then compute an sdf containing the business name, state, avg_rating, and max_avg_rating for businesses in states where that business is *not* most highly rated. Order the output in order of business name, decreasing avg_rating, and increasing state name. \n",
        "\n",
        "Convert the top 100 rows to Pandas and submit `below_avg_states_df` to the autograder.\n",
        "\n",
        "Your table should look something like:\n",
        ">name | state |avg_rating | max_avg_rating\n",
        ">--- | --- |--- | ---\n",
        ">business name 1 | state 1|  rating 1 | maxing rating 1\n",
        ">business name 1 | state 2| rating 2 | maxing rating 1\n",
        ">business name 2 | state 3| rating 3 | maxing rating 2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X8U1EQebds6"
      },
      "source": [
        "below_avg_states_df = #TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpoocHBtfyrB"
      },
      "source": [
        "grader.grade(test_case_id = 'check_by_state_rating', answer = below_avg_states_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7auunIQG7hi2"
      },
      "source": [
        "###4.3 Format Yelp Data as a Graph\n",
        "\n",
        "\n",
        "The Yelp data you've been working with can be thought of as graph data. Recall that a graph is made up of a set of verticies that are connected by edges. Within the context of our data, we can think of the users/businesses as nodes. Edges would then represent a review by a user for a business.\n",
        "\n",
        "With this in mind, we now want to reformat the yelp_reviews dataset to look more like a graph. \n",
        "\n",
        "**TODO:** Use Spark SQL to rename the user_id column of yelp_reviews data to from_node and rename the business_id column to to_node. Filter to rows where both the user_id and business_id are not null.  Create a temporary view with this table.\n",
        "\n",
        "Your table should look something like:\n",
        ">from_node | to_node | score\n",
        ">--- | --- | ---\n",
        ">user id 1 | business id 1 | stars 1\n",
        ">user id 2 | business id 2 | stars 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JlSq4FCbbCh"
      },
      "source": [
        "review_graph_sdf = #TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36o1clK3jwzY"
      },
      "source": [
        "review_graph_sdf.show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Qz2_SEHYztC"
      },
      "source": [
        "**TODO:** Once you've made your graph and created a temporary view, use Spark SQL to filter to the rows in the graph that contain the sequence \"abc\" anywhere in the from_node. Convert this subset to a Pandas dataframe called named `review_graph_abc`  and submit this to the autograder.\n",
        "\n",
        "HINT: Look into the LIKE keyword and wildcards in SQL. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xWBs7KXW9Bo"
      },
      "source": [
        "review_graph_abc_sdf = #TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joCY97gtaiJi"
      },
      "source": [
        "# Add test case for making graph\n",
        "grader.grade(test_case_id = 'reviewGraphCheck', answer = review_graph_abc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA84jsFau2Ls"
      },
      "source": [
        "\n",
        "## Part 5. “Traversing” a Graph [21 points total]\n",
        "\n",
        "For our next tasks, we will be “walking” the graph and making connections.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrftJPvUiY59"
      },
      "source": [
        "\n",
        "### 5.1 Intro to Distributed Breadth-First Search\n",
        "\n",
        "\n",
        "Now that we have created our graph, we will be implementing a graph traversal algorithm known as Breadth First Search. It works in a way that's equivalent to how a stain spreads on a white t-shirt. Take a look at the graph below:\n",
        "\n",
        "<p align = \"center\">\n",
        "<img src = \"https://imgur.com/WU3AUwg.png\" width= \"600\" align =\"center\"/>\n",
        "\n",
        "* Consider starting BFS from point A (green). This is considered the starting frontier/singular origin node.\n",
        "* The first round of BFS would involve finding all the nodes directly reachable from A, namely B-F (blue circles). These blue nodes make up the next frontier at depth 1 away from our starting node A.\n",
        "* The second round would then be identifying the red nodes which are the neighbors of the blue nodes. Now, the red nodes all belong to a frontier 2 depth away from A.\n",
        "\n",
        "This process continues until all the nodes in the graph have been visited. \n",
        "\n",
        "\n",
        "If you would like to learn more about BFS, I highly suggest looking [here](https://www.tutorialspoint.com/data_structures_algorithms/breadth_first_traversal.html).\n",
        "\n",
        "\n",
        "We will now be implementing **spark_bfs(G, N, d)**, our spark flavor of BFS that takes a graph **G**, a set of origin nodes **N**, and a max depth **d**.\n",
        "\n",
        "In order to write a successful BFS function, you are going to need to figure out \n",
        "1. how to keep track of nodes that we have visited\n",
        "2. how to properly find all the nodes at the next depth\n",
        "3. how to avoid cycles and ensure that we do not constantly loop through the same edges (take a look at J-K in the graph)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG8yQrCLjegX"
      },
      "source": [
        "### 5.2 Implement one Traversal\n",
        "\n",
        "To break down this process, let's think about how we would implement a single traversal of the graph. That is given the green node in the graph above, how are we going to get the blue nodes?\n",
        "\n",
        "\n",
        "Consider the simple graph below **which is different from the graph in the image above**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uQoYThcBSdZ"
      },
      "source": [
        "# This is a pairs notation of the edges, for simplicity of visualization\n",
        "graph = [('A', 'B'),\n",
        "         ('A', 'C'),\n",
        "         ('A', 'D'),\n",
        "         ('C', 'F'),\n",
        "         ('F', 'A'),\n",
        "         ('B', 'G'),\n",
        "         ('G', 'H'),\n",
        "         ('D', 'E')]\n",
        "# Here's an equivalent dictionary representation that we can use for a\n",
        "# Pandas DataFrame...\n",
        "simple_dict = {'from_node': ['A', 'A', 'A', 'C', 'F', 'B', 'G', 'D'],\n",
        "       'to_node': ['B', 'C', 'D', 'F', 'A', 'G', 'H', 'E']}\n",
        "\n",
        "simple_graph_df = pd.DataFrame.from_dict(simple_dict)\n",
        "# Ordinarily we can use: simple_graph_sdf = spark.createDataFrame(simple_graph_df)\n",
        "# BUT there is a bug in this verison of Spark so we'll write the Pandas dataframe\n",
        "# to a CSV, then read it back. You'll need to do this throughout the assignment.\n",
        "pd.DataFrame(simple_graph_df).to_csv('graph.csv')\n",
        "simple_graph_sdf = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"graph.csv\")\n",
        "\n",
        "simple_graph_sdf.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHRMuWzgqykO"
      },
      "source": [
        "As you can see, each row of this dataframe represents an edge between two nodes Although the nodes are labeled \"from\" and \"to\", the edges are actually undirected, meaning that A-->B represents the same edge as B-->A.\n",
        "\n",
        "Let's define our starting node as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcs0x5KaJi_B"
      },
      "source": [
        "smallOrig = [{'node': 'A'}]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAhTWrTrvJRJ"
      },
      "source": [
        "Then, bfs with graph G, starting from smallOrig to depth 1, or  **spark_bfs(G, smallOrig, 1)** would output as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGq2lYQKvJ0S"
      },
      "source": [
        "simple_1_round_dict = {'node': ['F', 'B', 'D', 'C', 'A'],\n",
        "       'distance': [1, 1, 1, 1, 0]}\n",
        "simple_1_round_bfs_df = pd.DataFrame.from_dict(simple_1_round_dict)\n",
        "\n",
        "# AGAIN we need to work around the Pandas to Spark dataframe bug, so\n",
        "# we will write to a CSV and then spark.read it...\n",
        "simple_1_round_bfs_df.to_csv('round1.csv',index=False)\n",
        "simple_1_round_bfs_sdf = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"round1.csv\")\n",
        "simple_1_round_bfs_sdf.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjF7XBz9vcDD"
      },
      "source": [
        "As you can see, this dataframe logs each node with its corresponding distance away from A. Moreover, we also know that these nodes are **visited**. \n",
        "\n",
        "Hopefully, you can see how we can use our original graph and this new information to find the nodes at depth two. \n",
        "\n",
        "This is exactly what we will try to accomplish with **spark_bfs_1_round(visited_nodes)** which will ultimately be the inner function of **spark_bfs** that we use to perform exactly one traversal of a graph.\n",
        "\n",
        "**TODO**: Write **spark_bfs_1_round(visted_nodes)** that takes the currently dataframe of visited_nodes, performs one round of BFS, and returns an updated visited nodes dataframe. You should assume that a temporary sdf G already exists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Yz3Gz5FAtrW"
      },
      "source": [
        "def spark_bfs_1_round(visited_nodes):\n",
        "  \"\"\"\n",
        "  :param visited_nodes: dataframe with columns node and distance\n",
        "  :return: dataframe of updated visuted nodes, with columns node and distance\n",
        "  \"\"\"\n",
        "  \n",
        "  #TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z1LRfM4NMvV"
      },
      "source": [
        "Now, run the inner function on **simple_1_round_bfs_sdf** result of 1 round of BFS on simple graph and store the results in **simple_bfs_result**. This is ultimately what the output of BFS to depth 2 should look like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDboxYrZKH-l"
      },
      "source": [
        "simple_graph_sdf.createOrReplaceTempView('G')\n",
        "simple_bfs_result = #TODO\n",
        "simple_bfs_result.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD82GqyJNWGu"
      },
      "source": [
        "Convert this result to Pandas, sorted by the node, and submit it to the autograder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_vnk78_K9B1"
      },
      "source": [
        "simple_bfs_test = #TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKcVOdEdLOXZ"
      },
      "source": [
        "grader.grade(test_case_id = 'checksimpleBFS', answer = simple_bfs_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOnnxL65yssC"
      },
      "source": [
        "### 5.3 Full BFS Implemntation\n",
        "\n",
        "Now, we will fully implement **spark_bfs**. This function should iteratively call your implemented version of **spark_bfs_1_round** and ultimately return the output of this function at **max_depth**.\n",
        "\n",
        "You are also responsible for initializing the starting dataframe, that is converting the list of origin nodes into a spark dataframe with the nodes logged at distance 0.\n",
        "\n",
        "Consider the following: \n",
        "\n",
        "```\n",
        "schema = StructType([\n",
        "            StructField(\"node\", StringType(), True)\n",
        "        ])\n",
        "\n",
        "    my_sdf = spark.read.format(\"csv\").schema(schema).load(\"my.csv\")\n",
        "```\n",
        "\n",
        "The schema ultimately specifies the structure of the Spark DataFrame with a string `node` column. It then calls **spark.load** to read the CSV with this schema. Also, you are responsible for ensuring that a view of your graph is available within this function. (Note: you will also need to add in a distance column)\n",
        "\n",
        "**TODO:** implement **spark_bfs(G,origins,max_depth)** and run on **review_graph_sdf** initalized in 4.3. Note: you may want to run tests on the **simple_graph** example as the `review_graph_sdf` will take quite some time to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXkRvJXKiVum"
      },
      "source": [
        "# TODO: iterative search over undirected graph\n",
        "# Worth 5 points directly, but will be needed later\n",
        "\n",
        "def spark_bfs(G, origins, max_depth):\n",
        "  \"\"\" runs distributed BFS to a specified max depth\n",
        "\n",
        "  :param G: graph dataframe from 4.3\n",
        "  :param origins: list of origin nodes stored as {\"node\": nodeValue}\n",
        "  :param max_depth: integer value of max depth to run BFS to\n",
        "  :return: dataframe with columns node, distance of all visited nodes\n",
        "  \"\"\"\n",
        "\n",
        "  #TODO \n",
        "  # Remember that if you want to go from Pandas dataframes to Spark dataframes,\n",
        "  # you may need to write to a CSV and read it back."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFA-Al0MNutj"
      },
      "source": [
        "Test that this function works on the simple example first. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twqNOYGOM_5u"
      },
      "source": [
        "simple_bfs_iterative_result = spark_bfs(simple_graph_sdf, smallOrig, 3)\n",
        "simple_bfs_iterative_result.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYpE6Pp1njJu"
      },
      "source": [
        "**TODO**: Using the starting node defined below, create **bfs_3** as the result of running **sparkbfs** on **review_graph_sdf** to a depth of 3. Finally, create a pandas dataframe of the first 75 results sorted by id as **answer_75_df** and submit this to the autograder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVUwgLmhiiAY"
      },
      "source": [
        "orig = [{'node': 'bv2nCi5Qv5vroFiqKGopiw'}]\n",
        "#grab the count\n",
        "bfs_3 = #TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZwiin6fnV8d"
      },
      "source": [
        "answer_75_df = #TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZnMt_CKnouC"
      },
      "source": [
        "When submitting to the autograder, submit as a tuple where first value is the length of your output dataframe and the second is the first 75 rows of your result.\n",
        "\n",
        "However, before you grab your first 75 rows, sort by the ids"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAqdWoVtjDMI"
      },
      "source": [
        "#13603 is just obtained from running count.count()\n",
        "grader.grade(test_case_id = 'checkBFS', answer = (length, answer_75_df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgwWGKJR2h7C"
      },
      "source": [
        "Congratulations on making it to the end of Homework 2! We know this assignment was pretty dense, but we hope that you still managed to learn a lot from it :)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hKZ_uL8Tsgl"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO7ITxz1izyA"
      },
      "source": [
        "# HW Submission\n",
        "\n",
        "\n",
        "Before you submit on Gradescope (you must submit your notebook to receive credit):\n",
        "\n",
        "\n",
        "1.   Restart and Run-All to make sure there's nothing wrong with your notebook\n",
        "2.   **Double check that you have the correct PennID (all numbers) in the autograder**. \n",
        "3. Make sure you've run all the PennGrader cells\n",
        "4. Go to the \"File\" tab at the top left, and click \"Download .ipynb\" and then \"Download .py\".  **Rename** the files to \"homework2.ipynb\" and \"homework2.py\" respectively and upload them to Gradescope **through the Coursera LTI item.**\n",
        "\n",
        "**Let the course staff know ASAP if you have any issues submitting, but otherwise best of luck!**"
      ]
    }
  ]
}